diff --git a/config/registry.py b/config/registry.py
index 63bf524..cb38b22 100644
--- a/config/registry.py
+++ b/config/registry.py
@@ -204,8 +204,8 @@ class SignalComponents:
     """Signal component names - must match across all modules."""
     
     ALL_COMPONENTS = [
-        "options_flow",
-        "dark_pool", 
+        "flow",
+        "dark_pool",
         "insider",
         "iv_term_skew",
         "smile_slope",
@@ -219,12 +219,13 @@ class SignalComponents:
         "institutional",
         "market_tide",
         "calendar_catalyst",
-        "etf_flow",
         "greeks_gamma",
         "ftd_pressure",
         "iv_rank",
         "oi_change",
-        "squeeze_score"
+        "etf_flow",
+        "squeeze_score",
+        "freshness_factor"
     ]
     
     @classmethod
diff --git a/main.py b/main.py
index b6b4011..06933a6 100644
--- a/main.py
+++ b/main.py
@@ -1,4 +1,5 @@
 # main.py ΓÇö Single-file adaptive bot with comprehensive Unusual Whales integration + Alpaca paper trading
+# IMPORTANT: For project context, common issues, and solutions, see MEMORY_BANK.md
 # Features:
 # - Multi-factor scoring: flow clusters + dark pool + gamma/greeks + net premium + realized vol + option volume levels
 # - Disciplined thresholds and weights (configurable via env)
@@ -27,7 +28,7 @@ from flask import Flask, jsonify, Response, send_from_directory
 from position_reconciliation_loop import run_position_reconciliation_loop
 
 from config.registry import (
-    Directories, CacheFiles, StateFiles, LogFiles, Thresholds, APIConfig,
+    Directories, CacheFiles, StateFiles, LogFiles, ConfigFiles, Thresholds, APIConfig,
     read_json, atomic_write_json, append_jsonl
 )
 
@@ -45,6 +46,7 @@ from signals.uw_weight_tuner import UWWeightTuner, load_live_weights
 
 import uw_enrichment_v2 as uw_enrich
 import uw_composite_v2 as uw_v2
+from uw_composite_v2 import get_threshold
 import cross_asset_confirmation as cross_asset
 import uw_execution_v2 as uw_exec
 import feature_attribution_v2 as feat_attr
@@ -95,6 +97,98 @@ def get_exit_urgency(position_data: dict, current_signals: dict) -> dict:
         return optimizer.compute_exit_urgency(position_data, current_signals)
     return {"action": "HOLD", "urgency": 0.0}
 
+def build_composite_close_reason(exit_signals: dict) -> str:
+    """
+    Build composite close reason from multiple exit signals (like entry uses composite signals).
+    
+    Args:
+        exit_signals: Dict with exit signal components:
+            - time_exit: bool or age_hours
+            - trail_stop: bool or pnl_pct
+            - signal_decay: float (decay ratio)
+            - flow_reversal: bool
+            - profit_target: float (pct hit)
+            - drawdown: float (pct)
+            - momentum_reversal: bool
+            - regime_protection: str
+            - displacement: str (symbol)
+            - stale_position: bool
+    
+    Returns:
+        Composite reason string like: "time_exit(72h)+signal_decay(0.65)+flow_reversal"
+    """
+    reasons = []
+    
+    # Time-based exits
+    if exit_signals.get("time_exit"):
+        age_hours = exit_signals.get("age_hours", 0)
+        if age_hours > 0:
+            reasons.append(f"time_exit({age_hours:.0f}h)")
+        else:
+            reasons.append("time_exit")
+    
+    # Trail stop
+    if exit_signals.get("trail_stop"):
+        pnl_pct = exit_signals.get("pnl_pct", 0.0)
+        if pnl_pct < 0:
+            reasons.append(f"trail_stop({pnl_pct:.1f}%)")
+        else:
+            reasons.append("trail_stop")
+    
+    # Signal decay
+    signal_decay = exit_signals.get("signal_decay")
+    if signal_decay is not None and signal_decay < 1.0:
+        reasons.append(f"signal_decay({signal_decay:.2f})")
+    
+    # Flow reversal
+    if exit_signals.get("flow_reversal"):
+        reasons.append("flow_reversal")
+    
+    # Profit target
+    profit_target = exit_signals.get("profit_target")
+    if profit_target is not None and profit_target > 0:
+        reasons.append(f"profit_target({int(profit_target*100)}%)")
+    
+    # Drawdown
+    drawdown = exit_signals.get("drawdown")
+    if drawdown is not None and drawdown > 0:
+        reasons.append(f"drawdown({drawdown:.1f}%)")
+    
+    # Momentum reversal
+    if exit_signals.get("momentum_reversal"):
+        reasons.append("momentum_reversal")
+    
+    # Regime protection
+    regime = exit_signals.get("regime_protection")
+    if regime:
+        reasons.append(f"regime_{regime}")
+    
+    # Displacement
+    displacement = exit_signals.get("displacement")
+    if displacement:
+        reasons.append(f"displaced_by_{displacement}")
+    
+    # Stale position
+    if exit_signals.get("stale_position"):
+        reasons.append("stale_position")
+    
+    # If no specific reasons, use primary reason or default
+    if not reasons:
+        primary = exit_signals.get("primary_reason")
+        if primary and primary != "none" and primary != "unknown":
+            reasons.append(primary)
+        else:
+            # Default fallback - should never happen if exit_signals is populated correctly
+            reasons.append("unknown_exit")
+    
+    result = "+".join(reasons) if reasons else "unknown_exit"
+    
+    # Safety check: ensure we never return empty string
+    if not result or result.strip() == "":
+        result = "unknown_exit"
+    
+    return result
+
 from v2_nightly_orchestration_with_auto_promotion import should_run_direct_v2
 from telemetry.logger import TelemetryLogger, timestamp_to_iso
 from health_supervisor import get_supervisor
@@ -147,10 +241,16 @@ class Config:
     UW_API_KEY = get_env("UW_API_KEY")
     ALPACA_KEY = get_env("ALPACA_KEY")
     ALPACA_SECRET = get_env("ALPACA_SECRET")
-    ALPACA_BASE_URL = get_env("ALPACA_BASE_URL", "https://paper-api.alpaca.markets")
+    ALPACA_BASE_URL = get_env("ALPACA_BASE_URL", APIConfig.ALPACA_BASE_URL)
 
     # Runtime
     TRADING_MODE = get_env("TRADING_MODE", "PAPER")  # PAPER or LIVE - v3.1.1
+    # Live-trading arming gate (prevents accidental real-money trading)
+    # In LIVE mode, bot will refuse to place new entry orders unless explicitly acknowledged.
+    LIVE_TRADING_ACK = get_env("LIVE_TRADING_ACK", "")
+    REQUIRE_LIVE_ACK = get_env("REQUIRE_LIVE_ACK", "true").lower() == "true"
+    # Optional safety mode: block opening short positions (bearish entries).
+    LONG_ONLY = get_env("LONG_ONLY", "false").lower() == "true"
     RUN_INTERVAL_SEC = get_env("RUN_INTERVAL_SEC", 60, int)
     LOG_LEVEL = get_env("LOG_LEVEL", "INFO")
     API_PORT = get_env("API_PORT", 8080, int)
@@ -367,7 +467,7 @@ if Config.ENABLE_PER_TICKER_LEARNING:
 # Load theme risk config from persistent file (overrides env vars)
 def load_theme_risk_config():
     """Load theme risk settings from config/theme_risk.json with priority over env vars."""
-    config_path = "config/theme_risk.json"
+    config_path = ConfigFiles.THEME_RISK
     if os.path.exists(config_path):
         try:
             with open(config_path, 'r') as f:
@@ -421,6 +521,51 @@ def jsonl_write(name, record):
 def log_event(kind, msg, **kw):
     jsonl_write(kind, {"msg": msg, **kw})
 
+def _is_live_endpoint(url: str) -> bool:
+    try:
+        return "api.alpaca.markets" in (url or "") and "paper-api" not in (url or "")
+    except Exception:
+        return False
+
+def _is_paper_endpoint(url: str) -> bool:
+    try:
+        return "paper-api.alpaca.markets" in (url or "")
+    except Exception:
+        return False
+
+def trading_is_armed() -> bool:
+    """
+    Returns True if the bot is allowed to place NEW entry orders.
+    Exits and monitoring may still run when unarmed.
+    """
+    mode = (Config.TRADING_MODE or "PAPER").upper()
+    base_url = Config.ALPACA_BASE_URL or ""
+
+    # If LIVE but pointed at paper, refuse entries (misconfiguration).
+    if mode == "LIVE" and _is_paper_endpoint(base_url):
+        return False
+
+    # If PAPER but pointed at live, refuse entries (misconfiguration).
+    if mode == "PAPER" and _is_live_endpoint(base_url):
+        return False
+
+    if mode == "LIVE" and Config.REQUIRE_LIVE_ACK:
+        return (Config.LIVE_TRADING_ACK or "").strip() == "YES_I_UNDERSTAND"
+
+    return True
+
+def build_client_order_id(symbol: str, side: str, cluster: dict, suffix: str = "") -> str:
+    """
+    Build a deterministic-ish client_order_id for idempotency.
+    Uniqueness is scoped by (symbol, side, cluster start_ts) plus a suffix for retries.
+    """
+    try:
+        start_ts = cluster.get("start_ts") or cluster.get("ts") or int(time.time())
+    except Exception:
+        start_ts = int(time.time())
+    base = f"uwbot-{symbol}-{side}-{int(start_ts)}"
+    return f"{base}-{suffix}" if suffix else base
+
 def log_blocked_trade(symbol: str, reason: str, score: float, signals: dict = None, 
                       direction: str = None, decision_price: float = None, 
                       components: dict = None, **kw):
@@ -874,6 +1019,56 @@ def log_exit_attribution(symbol: str, info: dict, exit_price: float, close_reaso
         pnl_usd = 0.0
         pnl_pct = 0.0
     
+    # Ensure close_reason is never empty or None
+    if not close_reason or close_reason == "unknown" or close_reason.strip() == "":
+        # Fallback: create a basic close reason
+        close_reason = "unknown_exit"
+        log_event("exit", "close_reason_missing", symbol=symbol, 
+                 note="close_reason was empty, using fallback")
+    
+    # V4.0: Enhanced context for causal analysis - capture EVERYTHING that might explain win/loss
+    entry_dt = entry_ts if isinstance(entry_ts, datetime) else datetime.fromisoformat(str(entry_ts).replace("Z", "+00:00")) if isinstance(entry_ts, str) else datetime.now(timezone.utc)
+    if entry_dt.tzinfo is None:
+        entry_dt = entry_dt.replace(tzinfo=timezone.utc)
+    
+    hour = entry_dt.hour
+    if hour < 9 or hour >= 16:
+        time_of_day = "AFTER_HOURS"
+    elif hour == 9:
+        time_of_day = "OPEN"
+    elif hour >= 15:
+        time_of_day = "CLOSE"
+    else:
+        time_of_day = "MID_DAY"
+    
+    day_of_week = entry_dt.strftime("%A").upper()
+    
+    # Extract signal characteristics for causal analysis
+    components = info.get("components", {}) or metadata.get("components", {}) if metadata else {}
+    entry_score = info.get("entry_score", 0.0) or metadata.get("entry_score", 0.0) if metadata else 0.0
+    
+    # Flow magnitude
+    flow_conv = 0.0
+    if isinstance(components.get("flow"), dict):
+        flow_conv = components["flow"].get("conviction", 0.0)
+    elif isinstance(components.get("flow"), (int, float)):
+        flow_conv = float(components.get("flow", 0.0))
+    
+    if flow_conv < 0.3:
+        flow_magnitude = "LOW"
+    elif flow_conv < 0.7:
+        flow_magnitude = "MEDIUM"
+    else:
+        flow_magnitude = "HIGH"
+    
+    # Signal strength
+    if entry_score < 2.5:
+        signal_strength = "WEAK"
+    elif entry_score < 3.5:
+        signal_strength = "MODERATE"
+    else:
+        signal_strength = "STRONG"
+    
     context = {
         "close_reason": close_reason,
         "entry_price": round(entry_price, 4),
@@ -882,10 +1077,17 @@ def log_exit_attribution(symbol: str, info: dict, exit_price: float, close_reaso
         "hold_minutes": round(hold_minutes, 1),
         "side": side,
         "qty": qty,
-        "entry_score": info.get("entry_score", 0.0),
-        "components": info.get("components", {}),
-        "market_regime": info.get("market_regime", "unknown"),
-        "direction": info.get("direction", "unknown")
+        "entry_score": entry_score,
+        "components": components,
+        "market_regime": info.get("market_regime", "unknown") or (metadata.get("market_regime", "unknown") if metadata else "unknown"),
+        "direction": info.get("direction", "unknown") or (metadata.get("direction", "unknown") if metadata else "unknown"),
+        # V4.0: Enhanced context for causal analysis
+        "time_of_day": time_of_day,
+        "day_of_week": day_of_week,
+        "entry_hour": hour,
+        "flow_magnitude": flow_magnitude,
+        "signal_strength": signal_strength,
+        "entry_ts": entry_dt.isoformat(),
     }
     
     if metadata:
@@ -911,6 +1113,67 @@ def log_exit_attribution(symbol: str, info: dict, exit_price: float, close_reaso
               pnl_pct=round(pnl_pct, 2),
               hold_min=round(hold_minutes, 1),
               reason=close_reason)
+    
+    # SHORT-TERM LEARNING: Immediate learning after trade close
+    # This enables fast adaptation to market changes
+    try:
+        from comprehensive_learning_orchestrator_v2 import learn_from_trade_close
+        
+        comps = context.get("components", {})
+        regime = context.get("market_regime", "unknown")
+        sector = "unknown"  # Could extract from symbol if needed
+        
+        # Immediate learning from this trade
+        learn_from_trade_close(symbol, pnl_pct, comps, regime, sector)
+        
+        # Also feed to exit model for exit signal learning
+        from adaptive_signal_optimizer import get_optimizer
+        optimizer = get_optimizer()
+        if optimizer and hasattr(optimizer, 'exit_model'):
+            # Parse close reason to extract exit signals
+            exit_signals = []
+            if close_reason and close_reason != "unknown":
+                for part in close_reason.split("+"):
+                    part = part.strip()
+                    if "(" in part:
+                        signal_name = part.split("(")[0].strip()
+                    else:
+                        signal_name = part.strip()
+                    if signal_name:
+                        exit_signals.append(signal_name)
+            
+            # Map exit signals to exit model components
+            exit_components = {}
+            for signal in exit_signals:
+                if "signal_decay" in signal or "entry_decay" in signal:
+                    exit_components["entry_decay"] = 1.0
+                elif "flow_reversal" in signal or "adverse_flow" in signal:
+                    exit_components["adverse_flow"] = 1.0
+                elif "drawdown" in signal:
+                    exit_components["drawdown_velocity"] = 1.0
+                elif "time" in signal or "stale" in signal:
+                    exit_components["time_decay"] = 1.0
+                elif "momentum" in signal:
+                    exit_components["momentum_reversal"] = 1.0
+            
+            # Record exit outcome for learning
+            if exit_components and pnl_pct != 0:
+                if hasattr(optimizer, 'learner') and hasattr(optimizer.learner, 'record_trade_outcome'):
+                    optimizer.learner.record_trade_outcome(
+                        trade_data={
+                            "entry_ts": entry_ts.isoformat() if hasattr(entry_ts, 'isoformat') else str(entry_ts),
+                            "exit_ts": now_aware.isoformat(),
+                            "direction": context.get("direction", "unknown"),
+                            "close_reason": close_reason
+                        },
+                        feature_vector=exit_components,
+                        pnl=pnl_pct / 100.0,  # Convert % to decimal
+                        regime=regime,
+                        sector=sector
+                    )
+    except Exception as e:
+        # Don't fail exit logging if learning fails
+        log_event("exit", "learning_feed_failed", error=str(e))
 
 def compute_daily_metrics():
     path = os.path.join(LOG_DIR, "attribution.jsonl")
@@ -936,12 +1199,44 @@ def compute_daily_metrics():
 # =========================
 class UWClient:
     def __init__(self, api_key=None):
+        from config.registry import APIConfig
         self.api_key = api_key or Config.UW_API_KEY
-        self.base = "https://api.unusualwhales.com"
+        self.base = APIConfig.UW_BASE_URL
         self.headers = {"Authorization": f"Bearer {self.api_key}"} if self.api_key else {}
+    
+    def _to_iso(self, ts):
+        """Convert timestamp to ISO format."""
+        if ts is None:
+            from datetime import datetime
+            return datetime.utcnow().isoformat() + "Z"
+        if isinstance(ts, str):
+            return ts
+        try:
+            from datetime import datetime
+            if isinstance(ts, (int, float)):
+                return datetime.fromtimestamp(ts).isoformat() + "Z"
+        except:
+            pass
+        from datetime import datetime
+        return datetime.utcnow().isoformat() + "Z"
 
     def _get(self, path_or_url: str, params: dict = None) -> dict:
         url = path_or_url if path_or_url.startswith("http") else f"{self.base}{path_or_url}"
+        
+        # QUOTA TRACKING: Log all UW API calls for monitoring
+        quota_log = CacheFiles.UW_API_QUOTA
+        quota_log.parent.mkdir(parents=True, exist_ok=True)
+        try:
+            with quota_log.open("a") as f:
+                f.write(json.dumps({
+                    "ts": int(time.time()),
+                    "url": url,
+                    "params": params or {},
+                    "source": "UWClient"
+                }) + "\n")
+        except Exception:
+            pass  # Don't fail on quota logging
+        
         try:
             r = requests.get(url, headers=self.headers, params=params or {}, timeout=10)
             r.raise_for_status()
@@ -1394,7 +1689,7 @@ class SmartPoller:
     """
     def __init__(self):
         self.lock = threading.Lock()
-        self.state_file = Path("state/smart_poller.json")
+        self.state_file = StateFiles.SMART_POLLER
         self.intervals = {
             "option_flow": 60,        # Real-time: institutional trades (HIGH actionability)
             "top_net_impact": 300,    # 5min: aggregated net premium (MEDIUM actionability)
@@ -1553,7 +1848,7 @@ def owner_health_check() -> dict:
         issues.append({"check": "heartbeat_error", "error": str(e)})
     
     # 2. Check fail counter integrity
-    fail_counter_path = Path("state/fail_counter.json")
+    fail_counter_path = StateFiles.FAIL_COUNTER
     try:
         if fail_counter_path.exists():
             fc = json.loads(fail_counter_path.read_text())
@@ -1705,69 +2000,36 @@ def build_symbol_decisions(clusters, gex_map, dp_map, net_map, vol_map, ovl_map)
 # V3.2: Integrates with adaptive signal optimizer for global weight learning
 # =========================
 def learn_from_outcomes():
+    """
+    MEDIUM-TERM LEARNING: Daily batch processing of all data sources.
+    
+    Now uses comprehensive learning orchestrator to process:
+    - All historical trades (not just today's)
+    - Exit events
+    - Signal patterns
+    - Order execution quality
+    """
     if not Config.ENABLE_PER_TICKER_LEARNING:
         return
-    profiles = load_profiles()
-    today = datetime.utcnow().strftime("%Y-%m-%d")
-    path = os.path.join(LOG_DIR, "attribution.jsonl")
-    if not os.path.exists(path):
-        return
-    
-    trades_processed = 0
-    with open(path, "r", encoding="utf-8") as f:
-        for line in f:
-            rec = json.loads(line)
-            if rec.get("type") != "attribution":
-                continue
-            if not rec.get("ts", "").startswith(today):
-                continue
-            symbol = rec.get("symbol")
-            ctx = rec.get("context", {})
-            reward = float(rec.get("pnl_usd", 0))
-            entry_action = ctx.get("entry_action")
-            atr_mult = ctx.get("atr_mult")
-            comps = ctx.get("components", {})
-
-            prof = get_or_init_profile(profiles, symbol)
-            if entry_action:
-                prof["entry_bandit"] = update_bandit(prof["entry_bandit"], entry_action, reward)
-            stop_action = "atr_1.5x" if atr_mult == 1.5 else "atr_1.0x" if atr_mult == 1.0 else "atr_2.0x"
-            prof["stop_bandit"] = update_bandit(prof["stop_bandit"], stop_action, reward)
-
-            cw = prof.get("component_weights", dict(Config.DEFAULT_COMPONENT_WEIGHTS))
-            for k in Config.SIGNAL_COMPONENTS:
-                contrib = float(comps.get(k, 0.0))
-                if contrib == 0.0:
-                    continue
-                step = 0.05 * (1 if reward > 0 else -1)
-                mag = min(1.0, abs(contrib) / 2.0)
-                cw[k] = float(max(0.5, min(2.0, cw.get(k, 1.0) + step * mag)))
-            prof["component_weights"] = cw
-
-            prof["samples"] = prof.get("samples", 0) + 1
-            profiles[symbol] = prof
-            
-            # V3.2: Feed trade data to adaptive signal optimizer for global weight learning
-            regime = ctx.get("gamma_regime", "neutral")
-            sector = ctx.get("sector", "unknown")
-            record_trade_for_learning(comps, reward, regime, sector)
-            trades_processed += 1
     
-    save_profiles(profiles)
-    
-    # V3.2: Trigger adaptive weight update if enough trades processed
-    if trades_processed >= 5:
-        optimizer = _get_adaptive_optimizer()
-        if optimizer:
-            try:
-                result = optimizer.update_weights()
-                log_event("learning", "adaptive_weights_updated", 
-                         trades_processed=trades_processed,
-                         weights_updated=result.get("updated", 0))
-            except Exception as e:
-                log_event("learning", "adaptive_weights_update_failed", error=str(e))
-    
-    log_event("learning", "profiles_updated", trades_processed=trades_processed)
+    # Use comprehensive learning orchestrator
+    try:
+        from comprehensive_learning_orchestrator_v2 import run_daily_learning
+        results = run_daily_learning()
+        
+        log_event("learning", "comprehensive_learning_completed",
+                 attribution=results.get("attribution", 0),
+                 exits=results.get("exits", 0),
+                 signals=results.get("signals", 0),
+                 orders=results.get("orders", 0),
+                 weights_updated=results.get("weights_updated", 0))
+    except ImportError:
+        # Learning system not available - log but don't fail
+        log_event("learning", "comprehensive_learning_not_available", 
+                 note="comprehensive_learning_orchestrator_v2 not available")
+    except Exception as e:
+        log_event("learning", "comprehensive_learning_failed", error=str(e))
+        # Don't fallback to legacy - v2 is the only learning system
 
 def weekly_retrain_profiles():
     if not Config.ENABLE_PER_TICKER_LEARNING:
@@ -2564,7 +2826,13 @@ class AlpacaExecutor:
             return round(mid, 4)
         return None
 
-    def submit_entry(self, symbol: str, qty: int, side: str, regime: str = "unknown"):
+    def _get_order_by_client_order_id(self, client_order_id: str):
+        fn = getattr(self.api, "get_order_by_client_order_id", None)
+        if callable(fn):
+            return fn(client_order_id)
+        return None
+
+    def submit_entry(self, symbol: str, qty: int, side: str, regime: str = "unknown", client_order_id_base: str = None):
         """
         Submit entry order with spread watchdog and regime-aware execution.
         
@@ -2575,7 +2843,7 @@ class AlpacaExecutor:
         ref_price = self.get_last_trade(symbol)
         if ref_price <= 0:
             log_event("submit_entry", "bad_ref_price", symbol=symbol, ref_price=ref_price)
-            return None, None, "error"
+            return None, None, "error", 0, "bad_ref_price"
         
         # === SPREAD WATCHDOG (Audit Recommendation) ===
         if Config.ENABLE_SPREAD_WATCHDOG:
@@ -2588,15 +2856,16 @@ class AlpacaExecutor:
                              symbol=symbol, spread_bps=round(spread_bps, 1),
                              max_spread_bps=Config.MAX_SPREAD_BPS,
                              bid=bid, ask=ask)
-                    return None, None, "spread_too_wide"
+                    return None, None, "spread_too_wide", 0, "spread_too_wide"
         
         notional = qty * ref_price
         if notional < Config.MIN_NOTIONAL_USD:
             log_event("submit_entry", "min_notional_blocked", 
                      symbol=symbol, qty=qty, ref_price=ref_price, 
                      notional=notional, min_required=Config.MIN_NOTIONAL_USD)
-            return None, None, "min_notional_blocked"
+            return None, None, "min_notional_blocked", 0, "min_notional_blocked"
         
+        # RISK MANAGEMENT: Order size validation (enhanced version of existing check)
         try:
             acct = self.api.get_account()
             dtbp = float(acct.daytrading_buying_power)
@@ -2606,13 +2875,27 @@ class AlpacaExecutor:
             # Use regular buying_power for paper trading (dtbp is unreliable in paper accounts)
             available_bp = bp
             
+            # Enhanced validation using risk management module
+            try:
+                from risk_management import validate_order_size
+                order_valid, order_error = validate_order_size(symbol, qty, side, ref_price, available_bp)
+                if not order_valid:
+                    log_event("submit_entry", "risk_validation_blocked",
+                             symbol=symbol, side=side, qty=qty, notional=notional,
+                             error=order_error)
+                    return None, None, "risk_validation_failed", 0, order_error
+            except ImportError:
+                # Risk management not available - use existing check
+                pass
+            
+            # Existing buying power check (keep for backward compatibility)
             if required_margin > available_bp:
                 log_event("submit_entry", "insufficient_buying_power",
                          symbol=symbol, side=side, qty=qty, notional=notional,
                          required_margin=round(required_margin, 2),
                          available_dtbp=round(dtbp, 2),
                          available_bp=round(bp, 2))
-                return None, None, "insufficient_buying_power"
+                return None, None, "insufficient_buying_power", 0, "insufficient_buying_power"
         except Exception as e:
             log_event("submit_entry", "margin_check_failed", symbol=symbol, error=str(e))
         
@@ -2637,6 +2920,17 @@ class AlpacaExecutor:
         if limit_price is not None and Config.ENTRY_POST_ONLY:
             for attempt in range(1, Config.ENTRY_MAX_RETRIES + 1):
                 try:
+                    # Use idempotency key from risk management if available
+                    if client_order_id_base and len(client_order_id_base) > 0:
+                        client_order_id = f"{client_order_id_base}-lpo-a{attempt}"
+                    else:
+                        # Fallback: generate new idempotency key
+                        try:
+                            from risk_management import generate_idempotency_key
+                            client_order_id = generate_idempotency_key(symbol, side, qty)
+                        except ImportError:
+                            client_order_id = None
+                    
                     o = self.api.submit_order(
                         symbol=symbol,
                         qty=qty,
@@ -2644,19 +2938,26 @@ class AlpacaExecutor:
                         type="limit",
                         time_in_force="day",
                         limit_price=str(limit_price),
-                        extended_hours=False
+                        extended_hours=False,
+                        client_order_id=client_order_id
                     )
                     order_id = getattr(o, "id", None)
                     if order_id:
                         filled, filled_qty, filled_price = self.check_order_filled(order_id)
-                        if filled:
+                        if filled and filled_qty > 0:
+                            # If partial fill, cancel remainder and proceed with filled_qty only.
+                            if filled_qty < qty:
+                                try:
+                                    self.api.cancel_order(order_id)
+                                except Exception:
+                                    pass
                             log_order({"action": "submit_limit_filled", "symbol": symbol, "side": side,
                                        "limit_price": limit_price, "filled_price": filled_price, "attempt": attempt})
                             telemetry.log_order_event(
                                 event_type="LIMIT_FILLED",
                                 symbol=symbol,
                                 side=side,
-                                qty=qty,
+                                qty=filled_qty,
                                 order_type="limit",
                                 limit_price=limit_price,
                                 fill_price=filled_price,
@@ -2664,7 +2965,7 @@ class AlpacaExecutor:
                                 attempt=attempt,
                                 status="filled"
                             )
-                            return o, filled_price, "limit"
+                            return o, filled_price, "limit", filled_qty, "filled"
                         try:
                             self.api.cancel_order(order_id)
                         except Exception:
@@ -2672,6 +2973,18 @@ class AlpacaExecutor:
                     log_order({"action": "limit_not_filled", "symbol": symbol, "side": side,
                                "limit_price": limit_price, "attempt": attempt})
                 except Exception as e:
+                    # Idempotency: if the client_order_id already exists, fetch the existing order.
+                    if client_order_id_base:
+                        try:
+                            existing = self._get_order_by_client_order_id(f"{client_order_id_base}-lpo-a{attempt}")
+                            if existing is not None:
+                                existing_id = getattr(existing, "id", None)
+                                if existing_id:
+                                    filled, filled_qty, filled_price = self.check_order_filled(existing_id)
+                                    if filled and filled_qty > 0:
+                                        return existing, filled_price, "limit", filled_qty, "filled"
+                        except Exception:
+                            pass
                     log_order({"action": "limit_retry_failed", "symbol": symbol, "side": side,
                                "limit_price": limit_price, "attempt": attempt, "error": str(e)})
                 
@@ -2688,6 +3001,17 @@ class AlpacaExecutor:
 
         if limit_price is not None:
             try:
+                # Use idempotency key from risk management if available
+                if client_order_id_base and len(client_order_id_base) > 0:
+                    client_order_id = f"{client_order_id_base}-lpfinal"
+                else:
+                    # Fallback: generate new idempotency key
+                    try:
+                        from risk_management import generate_idempotency_key
+                        client_order_id = generate_idempotency_key(symbol, side, qty)
+                    except ImportError:
+                        client_order_id = None
+                
                 o = self.api.submit_order(
                     symbol=symbol,
                     qty=qty,
@@ -2695,19 +3019,25 @@ class AlpacaExecutor:
                     type="limit",
                     time_in_force="day",
                     limit_price=str(limit_price),
-                    extended_hours=False
+                    extended_hours=False,
+                    client_order_id=client_order_id
                 )
                 order_id = getattr(o, "id", None)
                 if order_id:
                     filled, filled_qty, filled_price = self.check_order_filled(order_id)
-                    if filled:
+                    if filled and filled_qty > 0:
+                        if filled_qty < qty:
+                            try:
+                                self.api.cancel_order(order_id)
+                            except Exception:
+                                pass
                         log_order({"action": "submit_limit_final_filled", "symbol": symbol, "side": side,
                                    "limit_price": limit_price, "filled_price": filled_price})
                         telemetry.log_order_event(
                             event_type="LIMIT_FINAL_FILLED",
                             symbol=symbol,
                             side=side,
-                            qty=qty,
+                            qty=filled_qty,
                             order_type="limit",
                             limit_price=limit_price,
                             fill_price=filled_price,
@@ -2715,7 +3045,7 @@ class AlpacaExecutor:
                             attempt="final",
                             status="filled"
                         )
-                        return o, filled_price, "limit"
+                        return o, filled_price, "limit", filled_qty, "filled"
                     try:
                         self.api.cancel_order(order_id)
                     except Exception:
@@ -2723,28 +3053,51 @@ class AlpacaExecutor:
                 log_order({"action": "limit_final_not_filled", "symbol": symbol, "side": side,
                            "limit_price": limit_price})
             except Exception as e:
+                if client_order_id_base:
+                    try:
+                        existing = self._get_order_by_client_order_id(f"{client_order_id_base}-lpfinal")
+                        if existing is not None:
+                            existing_id = getattr(existing, "id", None)
+                            if existing_id:
+                                filled, filled_qty, filled_price = self.check_order_filled(existing_id)
+                                if filled and filled_qty > 0:
+                                    return existing, filled_price, "limit", filled_qty, "filled"
+                    except Exception:
+                        pass
                 log_order({"action": "limit_final_failed", "symbol": symbol, "side": side,
                            "limit_price": limit_price, "error": str(e)})
 
         try:
+            # Use idempotency key from risk management if available
+            if client_order_id_base and len(client_order_id_base) > 0:
+                client_order_id = f"{client_order_id_base}-mkt"
+            else:
+                # Fallback: generate new idempotency key
+                try:
+                    from risk_management import generate_idempotency_key
+                    client_order_id = generate_idempotency_key(symbol, side, qty)
+                except ImportError:
+                    client_order_id = None
+            
             o = self.api.submit_order(
                 symbol=symbol,
                 qty=qty,
                 side=side,
                 type="market",
                 time_in_force="day",
-                extended_hours=False
+                extended_hours=False,
+                client_order_id=client_order_id
             )
             log_order({"action": "submit_market_fallback", "symbol": symbol, "side": side})
             order_id = getattr(o, "id", None)
             if order_id:
                 filled, filled_qty, filled_price = self.check_order_filled(order_id, max_wait_sec=1.0)
-                if filled:
+                if filled and filled_qty > 0:
                     telemetry.log_order_event(
                         event_type="MARKET_FILLED",
                         symbol=symbol,
                         side=side,
-                        qty=qty,
+                        qty=filled_qty,
                         order_type="market",
                         limit_price=None,
                         fill_price=filled_price,
@@ -2752,11 +3105,23 @@ class AlpacaExecutor:
                         attempt="market_fallback",
                         status="filled"
                     )
-                    return o, filled_price, "market"
-            return o, None, "market"
+                    return o, filled_price, "market", filled_qty, "filled"
+            # Live-safety: if not confirmed filled, do NOT mark position open. Reconciliation will pick it up.
+            return o, None, "market", 0, "submitted_unfilled"
         except Exception as e:
+            if client_order_id_base:
+                try:
+                    existing = self._get_order_by_client_order_id(f"{client_order_id_base}-mkt")
+                    if existing is not None:
+                        existing_id = getattr(existing, "id", None)
+                        if existing_id:
+                            filled, filled_qty, filled_price = self.check_order_filled(existing_id, max_wait_sec=1.0)
+                            if filled and filled_qty > 0:
+                                return existing, filled_price, "market", filled_qty, "filled"
+                except Exception:
+                    pass
             log_order({"action": "market_fail", "symbol": symbol, "side": side, "error": str(e)})
-            return None, None, "error"
+            return None, None, "error", 0, "error"
 
     def can_open_new_position(self) -> bool:
         positions = self.api.list_positions()
@@ -2873,6 +3238,110 @@ class AlpacaExecutor:
             })
         
         if not candidates:
+            # Log why no candidates found for debugging
+            try:
+                positions = self.api.list_positions()
+                total_positions = len(positions)
+                reasons = {
+                    "too_young": 0,
+                    "pnl_too_high": 0,
+                    "score_advantage_insufficient": 0,
+                    "in_cooldown": 0
+                }
+                
+                # Detailed per-position breakdown
+                position_details = []
+                
+                for pos in positions:
+                    symbol = getattr(pos, "symbol", "")
+                    if not symbol or symbol == new_symbol:
+                        continue
+                    
+                    # Get position details
+                    entry_price = float(getattr(pos, "avg_entry_price", 0))
+                    current_price = float(getattr(pos, "current_price", 0))
+                    if entry_price <= 0 or current_price <= 0:
+                        continue
+                    
+                    pnl_pct = (current_price - entry_price) / entry_price
+                    pos_meta = metadata.get(symbol, {})
+                    entry_ts_str = pos_meta.get("entry_ts")
+                    
+                    if entry_ts_str:
+                        try:
+                            entry_ts = datetime.fromisoformat(entry_ts_str)
+                            age_hours = (now - entry_ts).total_seconds() / 3600
+                        except Exception:
+                            age_hours = 0
+                    else:
+                        if symbol in self.opens:
+                            age_hours = (now - self.opens[symbol]["ts"]).total_seconds() / 3600
+                        else:
+                            age_hours = 0
+                    
+                    original_score = pos_meta.get("entry_score", 0)
+                    score_advantage = new_signal_score - original_score
+                    
+                    # Check cooldown
+                    cooldown_ts = displacement_cooldowns.get(symbol)
+                    in_cooldown = False
+                    if cooldown_ts:
+                        try:
+                            cooldown_dt = datetime.fromisoformat(cooldown_ts)
+                            if now < cooldown_dt + timedelta(hours=Config.DISPLACEMENT_COOLDOWN_HOURS):
+                                reasons["in_cooldown"] += 1
+                                in_cooldown = True
+                        except Exception:
+                            pass
+                    
+                    # Determine why this position is not eligible
+                    fail_reason = None
+                    if in_cooldown:
+                        fail_reason = "in_cooldown"
+                    elif age_hours < Config.DISPLACEMENT_MIN_AGE_HOURS:
+                        reasons["too_young"] += 1
+                        fail_reason = "too_young"
+                    elif abs(pnl_pct) > Config.DISPLACEMENT_MAX_PNL_PCT:
+                        reasons["pnl_too_high"] += 1
+                        fail_reason = "pnl_too_high"
+                    elif score_advantage < Config.DISPLACEMENT_SCORE_ADVANTAGE:
+                        reasons["score_advantage_insufficient"] += 1
+                        fail_reason = "score_advantage_insufficient"
+                    
+                    # Store detailed info for logging
+                    position_details.append({
+                        "symbol": symbol,
+                        "age_hours": round(age_hours, 2),
+                        "pnl_pct": round(pnl_pct * 100, 2),
+                        "original_score": round(original_score, 2),
+                        "score_advantage": round(score_advantage, 2),
+                        "fail_reason": fail_reason
+                    })
+                
+                # Log summary with detailed breakdown
+                log_event("displacement", "no_candidates_found",
+                         new_signal_score=round(new_signal_score, 2),
+                         total_positions=total_positions,
+                         reasons=reasons,
+                         min_age_hours=Config.DISPLACEMENT_MIN_AGE_HOURS,
+                         max_pnl_pct=Config.DISPLACEMENT_MAX_PNL_PCT,
+                         required_score_advantage=Config.DISPLACEMENT_SCORE_ADVANTAGE,
+                         position_details=position_details[:10])  # Log first 10 positions
+                
+                # Also print to console for immediate visibility
+                print(f"DEBUG DISPLACEMENT: No candidates found for score {new_signal_score:.2f}", flush=True)
+                print(f"  Total positions: {total_positions}", flush=True)
+                print(f"  Reasons: {reasons}", flush=True)
+                if position_details:
+                    print(f"  Sample positions:", flush=True)
+                    for pd in position_details[:5]:
+                        print(f"    {pd['symbol']}: age={pd['age_hours']:.1f}h, pnl={pd['pnl_pct']:.2f}%, "
+                              f"orig_score={pd['original_score']:.2f}, advantage={pd['score_advantage']:.2f}, "
+                              f"fail={pd['fail_reason']}", flush=True)
+            except Exception as e:
+                log_event("displacement", "diagnostic_failed", error=str(e))
+                print(f"DEBUG DISPLACEMENT: Diagnostic failed: {e}", flush=True)
+            
             return None
         
         # Sort by: worst P&L first, then oldest, then lowest original score
@@ -2923,11 +3392,18 @@ class AlpacaExecutor:
             except:
                 symbol_metadata = {}
             
+            # Build composite close reason for displacement
+            displacement_signals = {
+                "displacement": new_symbol,
+                "age_hours": (datetime.utcnow() - info.get("ts", datetime.utcnow())).total_seconds() / 3600.0
+            }
+            close_reason = build_composite_close_reason(displacement_signals)
+            
             log_exit_attribution(
                 symbol=symbol,
                 info=info,
                 exit_price=exit_price,
-                close_reason=f"displaced_by_{new_symbol}",
+                close_reason=close_reason,
                 metadata=symbol_metadata
             )
             
@@ -3011,6 +3487,10 @@ class AlpacaExecutor:
                 "updated_at": datetime.utcnow().isoformat()
             }
             
+            # V3.0: Persist targets if position is already open
+            if symbol in self.opens and "targets" in self.opens[symbol]:
+                metadata[symbol]["targets"] = self.opens[symbol]["targets"]
+            
             atomic_write_json(metadata_path, metadata)
             
         except Exception as e:
@@ -3071,6 +3551,15 @@ class AlpacaExecutor:
                             self.opens[symbol]["entry_score"] = meta.get("entry_score", 0.0)
                         if "components" not in self.opens[symbol] or not self.opens[symbol]["components"]:
                             self.opens[symbol]["components"] = meta.get("components", {})
+                        # V3.0: Restore targets from metadata if available
+                        if "targets" in meta and meta["targets"]:
+                            self.opens[symbol]["targets"] = meta["targets"]
+                        elif "targets" not in self.opens[symbol]:
+                            # Initialize targets if missing
+                            self.opens[symbol]["targets"] = [
+                                {"pct": t, "hit": False, "fraction": Config.SCALE_OUT_FRACTIONS[i] if i < len(Config.SCALE_OUT_FRACTIONS) else 0.0}
+                                for i, t in enumerate(Config.PROFIT_TARGETS)
+                            ]
             
             # Add any positions in metadata that aren't in self.opens
             for symbol, meta in metadata.items():
@@ -3086,6 +3575,16 @@ class AlpacaExecutor:
                     except:
                         entry_ts = datetime.utcnow()
                     
+                    # V3.0: Restore targets from metadata if available, otherwise initialize fresh
+                    targets_from_meta = meta.get("targets")
+                    if targets_from_meta:
+                        targets_state = targets_from_meta
+                    else:
+                        targets_state = [
+                            {"pct": t, "hit": False, "fraction": Config.SCALE_OUT_FRACTIONS[i] if i < len(Config.SCALE_OUT_FRACTIONS) else 0.0}
+                            for i, t in enumerate(Config.PROFIT_TARGETS)
+                        ]
+                    
                     self.opens[symbol] = {
                         "ts": entry_ts,
                         "entry_price": avg_entry,
@@ -3093,11 +3592,7 @@ class AlpacaExecutor:
                         "side": side,
                         "trail_dist": None,
                         "high_water": current_price,
-                        "targets": [
-                            {"pct": 0.02, "fraction": 0.30, "hit": False},
-                            {"pct": 0.05, "fraction": 0.30, "hit": False},
-                            {"pct": 0.10, "fraction": 0.40, "hit": False}
-                        ]
+                        "targets": targets_state
                     }
                     self.high_water[symbol] = current_price
                     log_event("reload", "position_added_from_metadata", symbol=symbol)
@@ -3117,6 +3612,7 @@ class AlpacaExecutor:
         self.reload_positions_from_metadata()
         
         to_close = []
+        exit_reasons = {}  # Track composite exit reasons per symbol
         try:
             positions_index = {getattr(p, "symbol", ""): p for p in self.api.list_positions()}
         except Exception:
@@ -3128,8 +3624,13 @@ class AlpacaExecutor:
         except:
             all_metadata = {}
 
+        # Get current UW cache for signal evaluation
+        uw_cache = read_uw_cache()
+        current_regime_global = self._get_global_regime() or "mixed"
+
         now = datetime.utcnow()
         for symbol, info in list(self.opens.items()):
+            exit_signals = {}  # Collect all exit signals for this position
             try:
                 # FIX: Handle both offset-naive and offset-aware timestamps
                 entry_ts = info["ts"]
@@ -3138,6 +3639,8 @@ class AlpacaExecutor:
                 age_min = (now - entry_ts).total_seconds() / 60.0
                 age_days = age_min / (24 * 60)
                 age_hours = age_days * 24
+                exit_signals["age_hours"] = age_hours
+                
                 current_price = self.get_quote_price(symbol)
                 if current_price <= 0:
                     # FIX: Use entry price as fallback for after-hours exit evaluation
@@ -3152,6 +3655,26 @@ class AlpacaExecutor:
             high_water_price = info.get("high_water", current_price)
             pnl_pct = ((current_price - entry_price) / entry_price * 100) if entry_price > 0 else 0
             high_water_pct = ((high_water_price - entry_price) / entry_price * 100) if entry_price > 0 else 0
+            exit_signals["pnl_pct"] = pnl_pct
+            
+            # Get current composite score for signal decay detection
+            current_composite_score = 0.0
+            flow_reversal = False
+            try:
+                enriched = uw_cache.get(symbol, {})
+                if enriched:
+                    composite = uw_v2.compute_composite_score_v3(symbol, enriched, current_regime_global)
+                    if composite:
+                        current_composite_score = composite.get("score", 0.0)
+                        # Check for flow reversal
+                        flow_sent = enriched.get("sentiment", "NEUTRAL")
+                        entry_direction = info.get("direction", "unknown")
+                        if entry_direction == "bullish" and flow_sent == "BEARISH":
+                            flow_reversal = True
+                        elif entry_direction == "bearish" and flow_sent == "BULLISH":
+                            flow_reversal = True
+            except Exception:
+                pass  # If we can't get current score, continue with defaults
             
             # V3.2: Use adaptive exit urgency from optimizer
             position_data = {
@@ -3162,50 +3685,73 @@ class AlpacaExecutor:
                 "direction": "LONG" if info.get("side", "buy") == "buy" else "SHORT"
             }
             current_signals = {
-                "composite_score": 0.0,  # Would need to fetch current score
-                "flow_reversal": False,
+                "composite_score": current_composite_score,
+                "flow_reversal": flow_reversal,
                 "momentum": 0.0
             }
             
+            # Calculate signal decay
+            entry_score = info.get("entry_score", 3.0)
+            if entry_score > 0 and current_composite_score > 0:
+                decay_ratio = current_composite_score / entry_score
+                if decay_ratio < 1.0:
+                    exit_signals["signal_decay"] = decay_ratio
+            
+            exit_signals["flow_reversal"] = flow_reversal
+            
             # --- AUDIT DEC 2025: Manual Regime Safety Override ---
             # Ensures protection even if adaptive optimizer is missing
             # Try multiple sources: metadata (entry regime), in-memory info, or global regime state
             current_regime = (
                 all_metadata.get(symbol, {}).get("market_regime") or
                 info.get("market_regime") or
-                self._get_global_regime() or
+                current_regime_global or
                 "unknown"
             )
             if current_regime == "high_vol_neg_gamma":
                 if info.get("side", "buy") == "buy" and pnl_pct < -0.5:
+                    exit_signals["regime_protection"] = "neg_gamma"
+                    exit_reasons[symbol] = build_composite_close_reason(exit_signals)
                     log_event("exit", "regime_safety_trigger", 
                              symbol=symbol, 
                              regime=current_regime,
                              pnl_pct=round(pnl_pct, 2),
-                             reason="regime_neg_gamma_protection")
+                             reason=exit_reasons[symbol])
                     to_close.append(symbol)
                     continue
             # --- END Regime Safety Override ---
             
             exit_recommendation = get_exit_urgency(position_data, current_signals)
             
+            # Collect factors from exit recommendation
+            if exit_recommendation.get("contributing_factors"):
+                for factor in exit_recommendation.get("contributing_factors", []):
+                    if "drawdown" in factor.lower():
+                        exit_signals["drawdown"] = high_water_pct - pnl_pct
+                    elif "momentum" in factor.lower():
+                        exit_signals["momentum_reversal"] = True
+            
             # V3.2: Adaptive exit can trigger immediate close
             if exit_recommendation.get("action") == "EXIT" and exit_recommendation.get("urgency", 0) >= 0.8:
+                exit_signals["primary_reason"] = exit_recommendation.get("primary_reason", "adaptive_urgency")
+                exit_reasons[symbol] = build_composite_close_reason(exit_signals)
                 log_event("exit", "adaptive_exit_urgent", 
                          symbol=symbol,
                          urgency=exit_recommendation.get("urgency"),
-                         reason=exit_recommendation.get("reason", "adaptive_urgency"))
+                         reason=exit_reasons[symbol])
                 to_close.append(symbol)
                 continue
             
             # V3.3: Time-based exit for stale low-movement positions
             if age_days >= Config.TIME_EXIT_DAYS_STALE:
                 if abs(pnl_pct / 100) < Config.TIME_EXIT_STALE_PNL_THRESH_PCT:
+                    exit_signals["stale_position"] = True
+                    exit_reasons[symbol] = build_composite_close_reason(exit_signals)
                     log_event("exit", "time_exit_stale", 
                              symbol=symbol, 
                              age_days=round(age_days, 1),
                              pnl_pct=round(pnl_pct, 2),
-                             reason="position_stale_low_movement")
+                             reason=exit_reasons[symbol])
                     to_close.append(symbol)
                     continue
 
@@ -3218,12 +3764,33 @@ class AlpacaExecutor:
 
             stop_hit = current_price <= trail_stop
             time_hit = age_min >= Config.TIME_EXIT_MINUTES
+            
+            if stop_hit:
+                exit_signals["trail_stop"] = True
+            if time_hit:
+                exit_signals["time_exit"] = True
 
             ret_pct = _position_return_pct(info["entry_price"], current_price, info.get("side", "buy"))
+            
+            # V3.0: Ensure targets exist (re-initialize if missing)
+            if "targets" not in info or not info["targets"]:
+                info["targets"] = [
+                    {"pct": t, "hit": False, "fraction": Config.SCALE_OUT_FRACTIONS[i] if i < len(Config.SCALE_OUT_FRACTIONS) else 0.0}
+                    for i, t in enumerate(Config.PROFIT_TARGETS)
+                ]
+                log_event("exit", "profit_targets_reinitialized", symbol=symbol, ret_pct=round(ret_pct, 4))
+            
             for tgt in info.get("targets", []):
                 if not tgt["hit"] and ret_pct >= tgt["pct"]:
                     if self._scale_out_partial(symbol, tgt["fraction"], info.get("side", "buy")):
                         tgt["hit"] = True
+                        # V3.0: Persist updated targets to metadata
+                        self._persist_position_metadata(symbol, info.get("ts", datetime.utcnow()), 
+                                                        info.get("entry_price", 0.0), info.get("qty", 0),
+                                                        info.get("side", "buy"), info.get("entry_score", 0.0),
+                                                        info.get("components", {}), 
+                                                        info.get("market_regime", "unknown"),
+                                                        info.get("direction", "unknown"))
                         side = info.get("side", "buy")
                         entry_price = info.get("entry_price", 0.0)
                         qty = info.get("qty", 1)
@@ -3242,7 +3809,10 @@ class AlpacaExecutor:
                         symbol_metadata = all_metadata.get(symbol, {})
                         components = symbol_metadata.get("components", info.get("components", {}))
                         
-                        close_reason = f"profit_target_{int(tgt['pct']*100)}pct"
+                        # Build composite close reason for profit target
+                        scale_exit_signals = exit_signals.copy()
+                        scale_exit_signals["profit_target"] = tgt["pct"]
+                        close_reason = build_composite_close_reason(scale_exit_signals)
                         
                         jsonl_write("attribution", {
                             "type": "attribution",
@@ -3276,7 +3846,16 @@ class AlpacaExecutor:
                                   fraction=tgt["fraction"])
 
             if time_hit or stop_hit:
+                # Build composite close reason before adding to close list
+                # CRITICAL: Always set exit_reason when adding to close list
+                if symbol not in exit_reasons:
+                    exit_reasons[symbol] = build_composite_close_reason(exit_signals)
                 to_close.append(symbol)
+                print(f"DEBUG EXITS: {symbol} marked for close - time_hit={time_hit}, stop_hit={stop_hit}, age={age_min:.1f}min, reason={exit_reasons[symbol]}", flush=True)
+        
+        if to_close:
+            print(f"DEBUG EXITS: Found {len(to_close)} positions to close: {to_close}", flush=True)
+            log_event("exit", "positions_to_close", symbols=to_close, count=len(to_close))
         
         for symbol in to_close:
             try:
@@ -3291,15 +3870,33 @@ class AlpacaExecutor:
                 if exit_price <= 0:
                     exit_price = entry_price
                 
+                print(f"DEBUG EXITS: Closing {symbol} at {exit_price:.2f} (entry: {entry_price:.2f}, hold: {holding_period_min:.1f}min)", flush=True)
                 self.api.close_position(symbol)
-                log_order({"action": "close_position", "symbol": symbol, "reason": "time_or_trail"})
+                print(f"DEBUG EXITS: Successfully closed {symbol}", flush=True)
+                
+                # Use composite close reason if available, otherwise build one
+                close_reason = exit_reasons.get(symbol)
+                if not close_reason:
+                    # Fallback: build from basic signals
+                    # Calculate age for fallback (holding_period_min is already calculated above)
+                    age_hours_fallback = holding_period_min / 60.0
+                    basic_signals = {
+                        "time_exit": holding_period_min >= Config.TIME_EXIT_MINUTES,
+                        "trail_stop": exit_price < entry_price * (1 - Config.TRAILING_STOP_PCT / 100),
+                        "age_hours": age_hours_fallback
+                    }
+                    close_reason = build_composite_close_reason(basic_signals)
+                    # Log that we used fallback
+                    log_event("exit", "close_reason_fallback", symbol=symbol, reason=close_reason)
+                
+                log_order({"action": "close_position", "symbol": symbol, "reason": close_reason})
                 
                 symbol_metadata = all_metadata.get(symbol, {})
                 log_exit_attribution(
                     symbol=symbol,
                     info=info,
                     exit_price=exit_price,
-                    close_reason="time_or_trail",
+                    close_reason=close_reason,
                     metadata=symbol_metadata
                 )
                 
@@ -3395,9 +3992,16 @@ class StrategyEngine:
         
         print(f"DEBUG decide_and_execute: Processing {len(clusters_sorted)} clusters (sorted by strength), stage={system_stage}", flush=True)
         
+        if len(clusters_sorted) == 0:
+            print("ΓÜá∩╕Å  WARNING: decide_and_execute called with 0 clusters - no trades possible", flush=True)
+            return orders
+        
         for c in clusters_sorted:
             log_signal(c)
             symbol = c["ticker"]
+            direction = c.get("direction", "unknown")
+            score = c.get("composite_score", 0.0)
+            print(f"DEBUG {symbol}: Processing cluster - direction={direction}, score={score:.2f}, source={c.get('source', 'unknown')}", flush=True)
             gex = gex_map.get(symbol, {"gamma_regime": "unknown"})
             
             prof = get_or_init_profile(self.profiles, symbol) if Config.ENABLE_PER_TICKER_LEARNING else {}
@@ -3674,27 +4278,125 @@ class StrategyEngine:
                                   decision_price=ref_price_check,
                                   components=comps)
                 continue
+            
+            # RISK MANAGEMENT: Check exposure limits before placing order
+            try:
+                from risk_management import check_symbol_exposure, check_sector_exposure, get_risk_limits
+                
+                # Check symbol exposure
+                current_positions = []
+                try:
+                    alpaca_positions = self.executor.api.list_positions()
+                    for ap in alpaca_positions:
+                        current_positions.append(ap)
+                except Exception:
+                    pass
+                
+                if current_positions:
+                    account = self.executor.api.get_account()
+                    account_equity = float(account.equity)
+                    
+                    symbol_safe, symbol_reason = check_symbol_exposure(symbol, current_positions, account_equity)
+                    if not symbol_safe:
+                        print(f"DEBUG {symbol}: BLOCKED by symbol_exposure_limit", flush=True)
+                        log_event("risk_management", "symbol_exposure_blocked", symbol=symbol, reason=symbol_reason)
+                        log_blocked_trade(symbol, "symbol_exposure_limit", score,
+                                         direction=c.get("direction"),
+                                         decision_price=ref_price_check,
+                                         components=comps, reason=symbol_reason)
+                        continue
+                    
+                    sector_safe, sector_reason = check_sector_exposure(current_positions, account_equity)
+                    if not sector_safe:
+                        print(f"DEBUG {symbol}: BLOCKED by sector_exposure_limit", flush=True)
+                        log_event("risk_management", "sector_exposure_blocked", symbol=symbol, reason=sector_reason)
+                        log_blocked_trade(symbol, "sector_exposure_limit", score,
+                                         direction=c.get("direction"),
+                                         decision_price=ref_price_check,
+                                         components=comps, reason=sector_reason)
+                        continue
+            except ImportError:
+                # Risk management not available - continue without exposure checks
+                pass
+            except Exception as exp_error:
+                log_event("risk_management", "exposure_check_error", symbol=symbol, error=str(exp_error))
+                # Continue on error - don't block trading if exposure checks fail
 
             print(f"DEBUG {symbol}: PASSED ALL GATES! Calling submit_entry...", flush=True)
             
             side = "buy" if c["direction"] == "bullish" else "sell"
+            print(f"DEBUG {symbol}: Side determined: {side}, qty={qty}, ref_price={ref_price_check}", flush=True)
+            
+            # RISK MANAGEMENT: Validate order size before submission (qty already calculated above)
+            try:
+                from risk_management import validate_order_size
+                account = self.executor.api.get_account()
+                buying_power = float(account.buying_power)
+                current_price = ref_price_check
+                
+                order_valid, order_error = validate_order_size(symbol, qty, side, current_price, buying_power)
+                if not order_valid:
+                    print(f"DEBUG {symbol}: BLOCKED by order_validation: {order_error}", flush=True)
+                    log_event("risk_management", "order_validation_failed", 
+                             symbol=symbol, qty=qty, side=side, error=order_error)
+                    log_blocked_trade(symbol, "order_validation_failed", score,
+                                     direction=c.get("direction"),
+                                     decision_price=ref_price_check,
+                                     components=comps, validation_error=order_error)
+                    continue
+            except ImportError:
+                # Risk management not available - continue without validation
+                pass
+            except Exception as val_error:
+                log_event("risk_management", "order_validation_error", symbol=symbol, error=str(val_error))
+                # Continue on error
+            
             try:
                 old_mode = Config.ENTRY_MODE
                 
-                # V3.2 CHECKPOINT: ROUTE_ORDERS - Execution Router
-                router_config = v32.ExecutionRouter.load_config()
-                bid, ask = self.executor.get_nbbo(symbol)
-                spread_bps = ((ask - bid) / bid * 10000) if bid > 0 else 100
-                toxicity_score = 0.0  # TODO: Link to toxicity sentinel
-                recent_failures = 0  # TODO: Track per-symbol execution failures
+                # Generate idempotency key using risk management function
+                try:
+                    from risk_management import generate_idempotency_key
+                    client_order_id_base = generate_idempotency_key(symbol, side, qty)
+                except ImportError:
+                    # Fallback to existing method
+                    client_order_id_base = build_client_order_id(symbol, side, c)
                 
-                # v3.2.1: ExecutionRouter with telemetry
-                selected_strategy, strategy_params = v32.ExecutionRouter.select_strategy(
-                    ticker=symbol,
-                    regime=market_regime,
-                    spread_bps=spread_bps,
-                    toxicity=toxicity_score
-                )
+                # V3.2 CHECKPOINT: ROUTE_ORDERS - Execution Router
+                try:
+                    router_config = v32.ExecutionRouter.load_config()
+                    bid, ask = self.executor.get_nbbo(symbol)
+                    if bid <= 0 or ask <= 0:
+                        print(f"DEBUG {symbol}: WARNING - get_nbbo returned invalid bid/ask: bid={bid}, ask={ask}", flush=True)
+                        # Use last trade price as fallback
+                        last_price = self.executor.get_last_trade(symbol)
+                        if last_price > 0:
+                            bid, ask = last_price * 0.999, last_price * 1.001  # Small spread estimate
+                            print(f"DEBUG {symbol}: Using fallback bid/ask from last trade: bid={bid}, ask={ask}", flush=True)
+                        else:
+                            print(f"DEBUG {symbol}: ERROR - Cannot get valid price for {symbol}, skipping order", flush=True)
+                            log_order({"symbol": symbol, "qty": qty, "side": side, "error": "invalid_price_data", "bid": bid, "ask": ask})
+                            continue
+                    spread_bps = ((ask - bid) / bid * 10000) if bid > 0 else 100
+                    toxicity_score = 0.0  # TODO: Link to toxicity sentinel
+                    recent_failures = 0  # TODO: Track per-symbol execution failures
+                    
+                    # v3.2.1: ExecutionRouter with telemetry
+                    selected_strategy, strategy_params = v32.ExecutionRouter.select_strategy(
+                        ticker=symbol,
+                        regime=market_regime,
+                        spread_bps=spread_bps,
+                        toxicity=toxicity_score
+                    )
+                    print(f"DEBUG {symbol}: ExecutionRouter selected strategy={selected_strategy}, spread_bps={spread_bps:.1f}", flush=True)
+                except Exception as router_ex:
+                    import traceback
+                    print(f"DEBUG {symbol}: EXCEPTION in execution router setup: {str(router_ex)}", flush=True)
+                    print(f"DEBUG {symbol}: Traceback: {traceback.format_exc()}", flush=True)
+                    log_order({"symbol": symbol, "qty": qty, "side": side, "error": f"execution_router_exception: {str(router_ex)}"})
+                    # Use default strategy on error
+                    selected_strategy = "limit_offset"
+                    strategy_params = {}
                 
                 # Map strategy to ENTRY_MODE
                 strategy_mode_map = {
@@ -3720,28 +4422,114 @@ class StrategyEngine:
                     else:
                         Config.ENTRY_MODE = "MARKET_FALLBACK"
                 
-                res, limit_price, order_type = self.executor.submit_entry(symbol, qty, side, regime=market_regime)
+                # Capture expected price for basic TCA logging (best-effort).
+                expected_entry_price = None
+                try:
+                    expected_entry_price = self.executor.compute_entry_price(symbol, side)
+                    print(f"DEBUG {symbol}: Expected entry price computed: {expected_entry_price}", flush=True)
+                except Exception as price_ex:
+                    print(f"DEBUG {symbol}: WARNING - compute_entry_price failed: {str(price_ex)}", flush=True)
+                    expected_entry_price = None
+
+                # Long-only safety: do not open shorts in LONG_ONLY mode.
+                if Config.LONG_ONLY and side == "sell":
+                    print(f"DEBUG {symbol}: BLOCKED by LONG_ONLY mode (short entry not allowed)", flush=True)
+                    log_event("gate", "long_only_blocked_short_entry", symbol=symbol, score=score)
+                    log_blocked_trade(symbol, "long_only_blocked_short_entry", score,
+                                      direction=c.get("direction"),
+                                      decision_price=ref_price_check,
+                                      components=comps)
+                    continue
+
+                print(f"DEBUG {symbol}: Building client_order_id_base...", flush=True)
+                client_order_id_base = build_client_order_id(symbol, side, c)
+                print(f"DEBUG {symbol}: client_order_id_base={client_order_id_base}", flush=True)
+                
+                # CRITICAL: Add exception handling and logging around submit_entry
+                try:
+                    print(f"DEBUG {symbol}: About to call submit_entry with qty={qty}, side={side}, regime={market_regime}", flush=True)
+                    res, fill_price, order_type, filled_qty, entry_status = self.executor.submit_entry(
+                        symbol, qty, side, regime=market_regime, client_order_id_base=client_order_id_base
+                    )
+                    print(f"DEBUG {symbol}: submit_entry completed - res={res is not None}, order_type={order_type}, entry_status={entry_status}, filled_qty={filled_qty}", flush=True)
+                except Exception as submit_ex:
+                    import traceback
+                    print(f"DEBUG {symbol}: EXCEPTION in submit_entry: {str(submit_ex)}", flush=True)
+                    print(f"DEBUG {symbol}: Traceback: {traceback.format_exc()}", flush=True)
+                    log_order({"symbol": symbol, "qty": qty, "side": side, "error": f"submit_entry_exception: {str(submit_ex)}", "traceback": traceback.format_exc()})
+                    res, fill_price, order_type, filled_qty, entry_status = None, None, "error", 0, "error"
+                
                 Config.ENTRY_MODE = old_mode
                 
                 if res is None:
-                    log_order({"symbol": symbol, "qty": qty, "side": side, "error": "submit_entry_failed", "order_type": order_type})
+                    print(f"DEBUG {symbol}: submit_entry returned None - order submission failed (order_type={order_type}, entry_status={entry_status})", flush=True)
+                    log_order({"symbol": symbol, "qty": qty, "side": side, "error": "submit_entry_failed", "order_type": order_type, "entry_status": entry_status})
+                    continue
+
+                print(f"DEBUG {symbol}: submit_entry returned - order_type={order_type}, entry_status={entry_status}, filled_qty={filled_qty}, fill_price={fill_price}", flush=True)
+
+                # CRITICAL FIX: Accept orders that are successfully submitted, even if not immediately filled
+                # The reconciliation loop will pick up fills later. Only reject if order submission actually failed.
+                if entry_status in ("error", "spread_too_wide", "min_notional_blocked", "risk_validation_failed", "insufficient_buying_power", "bad_ref_price"):
+                    print(f"DEBUG {symbol}: Order REJECTED - submission failed with status={entry_status}", flush=True)
+                    log_event("order", "entry_submission_failed", symbol=symbol, side=side, status=entry_status,
+                              client_order_id=client_order_id_base, requested_qty=qty)
                     continue
-                price = limit_price if limit_price is not None else self.executor.get_quote_price(symbol)
-                self.executor.mark_open(symbol, price, atr_mult, side, qty, entry_score=score, components=comps, market_regime=market_regime, direction=c["direction"])
                 
-                telemetry.log_portfolio_event(
-                    event_type="POSITION_OPENED",
-                    symbol=symbol,
-                    side=side,
-                    qty=qty,
-                    entry_price=price,
-                    exit_price=None,
-                    realized_pnl=0.0,
-                    unrealized_pnl=0.0,
-                    holding_period_min=0,
-                    order_type=order_type,
-                    score=score
-                )
+                # Order was successfully submitted (may or may not be filled yet)
+                if entry_status == "filled" and filled_qty > 0:
+                    print(f"DEBUG {symbol}: Order IMMEDIATELY FILLED - qty={filled_qty}, price={fill_price}", flush=True)
+                else:
+                    print(f"DEBUG {symbol}: Order SUBMITTED (not yet filled) - status={entry_status}, will be tracked by reconciliation", flush=True)
+                    # For submitted but unfilled orders, reconciliation will handle them
+                    # We still process them but don't mark as open until filled
+
+                # CRITICAL FIX: Handle both filled and submitted orders
+                if entry_status == "filled" and filled_qty > 0:
+                    # Order was immediately filled - process normally
+                    exec_qty = int(filled_qty)
+                    exec_price = float(fill_price) if fill_price is not None else self.executor.get_quote_price(symbol)
+                    self.executor.mark_open(symbol, exec_price, atr_mult, side, exec_qty, entry_score=score,
+                                            components=comps, market_regime=market_regime, direction=c["direction"])
+                else:
+                    # Order was submitted but not yet filled - reconciliation will handle it
+                    # For now, we accept the order submission as successful
+                    # Reconciliation loop will pick up the fill and mark position open
+                    exec_qty = int(filled_qty) if filled_qty > 0 else qty  # Use filled qty if available, else requested
+                    exec_price = float(fill_price) if fill_price is not None else self.executor.get_quote_price(symbol)
+                    print(f"DEBUG {symbol}: Order submitted (status={entry_status}) - reconciliation will track fill", flush=True)
+                    log_event("order", "entry_submitted_pending_fill", symbol=symbol, side=side, 
+                             requested_qty=qty, filled_qty=filled_qty, order_type=order_type, 
+                             client_order_id=client_order_id_base, entry_status=entry_status)
+                    # Don't mark_open for unfilled orders - reconciliation will do that when fill occurs
+                    # But we still want to count this as a successful order submission
+                
+                # Only log POSITION_OPENED if order was actually filled
+                if entry_status == "filled" and filled_qty > 0:
+                    telemetry.log_portfolio_event(
+                        event_type="POSITION_OPENED",
+                        symbol=symbol,
+                        side=side,
+                        qty=exec_qty,
+                        entry_price=exec_price,
+                        exit_price=None,
+                        realized_pnl=0.0,
+                        unrealized_pnl=0.0,
+                        holding_period_min=0,
+                        order_type=order_type,
+                        score=score
+                    )
+                else:
+                    # Log order submission for unfilled orders
+                    telemetry.log_order_event(
+                        event_type="ORDER_SUBMITTED",
+                        symbol=symbol,
+                        side=side,
+                        qty=qty,
+                        order_type=order_type,
+                        status=entry_status,
+                        note="pending_fill_reconciliation"
+                    )
                 
                 context = {
                     "direction": c["direction"],
@@ -3762,15 +4550,35 @@ class StrategyEngine:
                 else:
                     context["confirm_score"] = confirm_map.get(symbol, 0.0)
                 
-                orders.append({"symbol": symbol, "qty": qty, "side": side, "score": score, "order_type": order_type})
-                new_positions_this_cycle += 1  # V3.2.1: Track new positions per cycle
-                log_order({"symbol": symbol, "qty": qty, "side": side, "score": score, "price": price, "order_type": order_type})
+                # Append to orders list for both filled and submitted orders
+                # This ensures we track all order attempts, not just immediate fills
+                orders.append({"symbol": symbol, "qty": exec_qty, "side": side, "score": score, 
+                              "order_type": order_type, "status": entry_status, "filled_qty": filled_qty})
+                
+                if entry_status == "filled" and filled_qty > 0:
+                    new_positions_this_cycle += 1  # V3.2.1: Track new positions per cycle
+                    log_order({"symbol": symbol, "qty": exec_qty, "side": side, "score": score, 
+                              "price": exec_price, "order_type": order_type, "status": "filled"})
+                else:
+                    log_order({"symbol": symbol, "qty": exec_qty, "side": side, "score": score, 
+                              "price": exec_price, "order_type": order_type, "status": entry_status, 
+                              "note": "submitted_pending_fill"})
                 log_attribution(trade_id=f"open_{symbol}_{now_iso()}", symbol=symbol, pnl_usd=0.0, context=context)
                 
+                # RISK MANAGEMENT: Update daily start equity if this is first trade of day
+                try:
+                    from risk_management import get_daily_start_equity, set_daily_start_equity
+                    if get_daily_start_equity() is None:
+                        # First trade today - set baseline
+                        account = self.executor.api.get_account()
+                        set_daily_start_equity(float(account.equity))
+                except Exception:
+                    pass  # Non-critical
+                
                 # V3.2 CHECKPOINT: POST_TRADE - TCA Feedback & Champion-Challenger
                 # Log execution quality for TCA feedback
-                if limit_price and price:
-                    slippage_pct = abs(price - limit_price) / limit_price if limit_price > 0 else 0
+                if expected_entry_price and exec_price:
+                    slippage_pct = abs(exec_price - expected_entry_price) / expected_entry_price if expected_entry_price > 0 else 0
                     v32.log_jsonl(v32.TCA_SUMMARY_LOG, {
                         "timestamp": datetime.utcnow().isoformat(),
                         "symbol": symbol,
@@ -3790,7 +4598,11 @@ class StrategyEngine:
                 
             except Exception as e:
                 import traceback
-                log_order({"symbol": symbol, "qty": qty, "side": side, "error": str(e), "traceback": traceback.format_exc()})
+                error_msg = str(e)
+                error_trace = traceback.format_exc()
+                print(f"DEBUG {symbol}: EXCEPTION in decide_and_execute: {error_msg}", flush=True)
+                print(f"DEBUG {symbol}: Full traceback:\n{error_trace}", flush=True)
+                log_order({"symbol": symbol, "qty": qty, "side": side, "error": error_msg, "traceback": error_trace})
         
         if Config.ENABLE_PER_TICKER_LEARNING:
             save_profiles(self.profiles)
@@ -3884,7 +4696,7 @@ def audit_seg(name, phase, extra=None):
     }
     if extra:
         event.update(extra)
-    gov_log = Path("data/governance_events.jsonl")
+    gov_log = CacheFiles.GOVERNANCE_EVENTS
     gov_log.parent.mkdir(exist_ok=True)
     with gov_log.open("a") as f:
         f.write(json.dumps(event) + "\n")
@@ -4002,6 +4814,35 @@ def run_once():
             print(f"ΓÜá∩╕Å  Position reconciliation V2 error: {reconcile_error}", flush=True)
             log_event("position_reconciliation_v2", "error", error=str(reconcile_error))
         
+        # RISK MANAGEMENT CHECKS: Account-level risk limits (after position reconciliation)
+        try:
+            from risk_management import run_risk_checks
+            account = engine.executor.api.get_account()
+            current_equity = float(account.equity)
+            positions = engine.executor.api.list_positions()
+            
+            risk_results = run_risk_checks(engine.executor.api, current_equity, positions)
+            
+            if not risk_results["safe_to_trade"]:
+                freeze_reason = risk_results.get("freeze_reason", "unknown_risk_check")
+                alerts_this_cycle.append(f"risk_limit_breach_{freeze_reason}")
+                print(f"≡ƒ¢æ RISK LIMIT BREACH: {freeze_reason} - Trading halted", flush=True)
+                log_event("risk_management", "freeze_activated", 
+                         reason=freeze_reason, 
+                         checks=risk_results.get("checks", {}))
+                # Return early - freeze will be caught by freeze check next cycle
+                return {"clusters": 0, "orders": 0, "risk_freeze": freeze_reason}
+            else:
+                log_event("risk_management", "checks_passed", 
+                         daily_pnl=risk_results["checks"].get("daily_loss", {}).get("daily_pnl", 0),
+                         drawdown_pct=risk_results["checks"].get("drawdown", {}).get("drawdown_pct", 0))
+        except ImportError:
+            # Risk management module not available - log but continue (for backward compatibility)
+            log_event("risk_management", "module_not_available", warning=True)
+        except Exception as risk_error:
+            log_event("risk_management", "check_error", error=str(risk_error))
+            # On error, continue but log - don't block trading if risk checks fail
+        
         # MONITORING GUARD 2: Check heartbeat staleness (v3.1.1: 30m threshold, PAPER mode)
         if not check_heartbeat_staleness(REQUIRED_HEARTBEAT_MODULES, max_age_minutes=30, trading_mode=Config.TRADING_MODE):
             alerts_this_cycle.append("heartbeat_stale")
@@ -4020,12 +4861,68 @@ def run_once():
             # CACHE MODE: Read all data from uw-daemon cache - NO API CALLS
             print(f"DEBUG: Using centralized UW cache ({len(uw_cache)} symbols)", flush=True)
             
-            # Build maps from cache data
+            # GRACEFUL DEGRADATION: Track if we're using stale data
+            current_time = time.time()
+            stale_threshold = 2 * 3600  # 2 hours
+            using_stale_data = False
+            fresh_data_count = 0
+            stale_data_count = 0
+            
+            # Build maps from cache data AND extract flow trades for clustering
             for ticker in Config.TICKERS:
                 cache_data = uw_cache.get(ticker, {})
                 if not cache_data or cache_data.get("simulated"):
                     continue
                 
+                # Check cache age for graceful degradation
+                last_update = cache_data.get("_last_update", 0)
+                age_sec = current_time - last_update if last_update else float('inf')
+                is_stale = age_sec > stale_threshold
+                
+                # CRITICAL: Extract raw flow trades from cache for clustering
+                # Daemon stores raw API trades in cache_data["flow_trades"]
+                # We need to normalize them (same as UWClient.get_option_flow does)
+                flow_trades_raw = cache_data.get("flow_trades", None)
+                if flow_trades_raw is None:
+                    # Key doesn't exist - daemon hasn't polled this ticker yet
+                    print(f"DEBUG: No flow_trades key in cache for {ticker} (daemon not polled yet)", flush=True)
+                elif flow_trades_raw:
+                    # Key exists and has data - use it even if stale (graceful degradation)
+                    if is_stale:
+                        using_stale_data = True
+                        stale_data_count += 1
+                        print(f"DEBUG: Using STALE cache for {ticker} ({int(age_sec/60)} min old) - {len(flow_trades_raw)} trades", flush=True)
+                    else:
+                        fresh_data_count += 1
+                        print(f"DEBUG: Found {len(flow_trades_raw)} raw trades for {ticker}", flush=True)
+                    
+                    # Normalize raw API trades to match main.py's expected format
+                    uw_client = UWClient()
+                    normalized_count = 0
+                    filtered_count = 0
+                    for raw_trade in flow_trades_raw:
+                        try:
+                            # Normalize using same logic as UWClient.get_option_flow
+                            normalized_trade = uw_client._normalize_flow_trade(raw_trade)
+                            normalized_count += 1
+                            # Apply base filter (premium, expiry, etc.)
+                            if base_filter(normalized_trade):
+                                all_trades.append(normalized_trade)
+                                filtered_count += 1
+                        except Exception as e:
+                            # Log normalization errors for debugging
+                            print(f"DEBUG: Failed to normalize trade for {ticker}: {e}", flush=True)
+                            continue
+                    if normalized_count > 0:
+                        print(f"DEBUG: {ticker}: {normalized_count} normalized, {filtered_count} passed filter", flush=True)
+                else:
+                    # Key exists but is empty array - API returned no trades (likely rate limited)
+                    # Check if we have older cache data we can use
+                    if is_stale:
+                        print(f"DEBUG: flow_trades empty for {ticker} (stale cache, {int(age_sec/60)} min old)", flush=True)
+                    else:
+                        print(f"DEBUG: flow_trades key exists for {ticker} but is empty (API returned 0 trades)", flush=True)
+                
                 # Extract data from cache for confirmation scoring
                 dp_data = cache_data.get("dark_pool", {})
                 dp_map[ticker] = [{"off_lit_volume": dp_data.get("total_premium", 0)}]
@@ -4045,50 +4942,111 @@ def run_once():
                 
                 ovl_map[ticker] = []
             
-            log_event("data_source", "cache_mode", cache_symbols=len(uw_cache), api_calls=0)
-        else:
-            # FALLBACK: Direct API calls only when cache is empty (rare/startup)
-            print("DEBUG: Cache empty, using direct API (fallback mode)", flush=True)
+            # Log graceful degradation status
+            if using_stale_data:
+                print(f"Γ£à GRACEFUL DEGRADATION: Using stale cache data ({stale_data_count} stale, {fresh_data_count} fresh)", flush=True)
+                log_event("uw_cache", "graceful_degradation_active", 
+                         stale_tickers=stale_data_count,
+                         fresh_tickers=fresh_data_count,
+                         note="Trading continues with cached data < 2 hours old")
             
-            poll_top_net = _smart_poller.should_poll("top_net_impact")
-            poll_flow = _smart_poller.should_poll("option_flow")
+            log_event("data_source", "cache_mode", cache_symbols=len(uw_cache), api_calls=0, 
+                     stale_data_used=using_stale_data, stale_count=stale_data_count, fresh_count=fresh_data_count)
             
-            if poll_top_net:
-                try:
-                    top_net = uw.get_top_net_impact(limit=100)
-                    net_map = {x["ticker"]: x for x in top_net}
-                    _smart_poller.record_success("top_net_impact")
-                except Exception as e:
-                    log_event("uw_error", "top_net_impact_failed", error=str(e))
-                    _smart_poller.record_error("top_net_impact")
+            # CRITICAL FIX: Even if flow_trades is empty, composite scoring can still generate trades
+            # from sentiment, conviction, dark_pool, insider data in cache
+            # This ensures trading continues even when API is rate limited or returns empty flow_trades
+            print(f"DEBUG: Cache mode active - composite scoring will run even if flow_trades empty ({len(all_trades)} trades from flow, {len(uw_cache)} symbols in cache)", flush=True)
+            print(f"DEBUG: Maps built: {len(dp_map)} dark_pool, {len(gex_map)} gamma, {len(net_map)} net_premium", flush=True)
+        else:
+            # GRACEFUL DEGRADATION: Cache empty or daemon not running
+            # Check if we have ANY cached data (even if stale) to use
+            print("ΓÜá∩╕Å  WARNING: UW cache empty or unavailable", flush=True)
             
-            print("DEBUG: Starting ticker loop (fallback)", flush=True)
-            for ticker in Config.TICKERS:
-                if poll_flow:
-                    try:
-                        flow = uw.get_option_flow(ticker, limit=100)
-                        trades = [t for t in flow if base_filter(t)]
-                        all_trades.extend(trades)
-                    except Exception as e:
-                        log_event("uw_error", "flow_fetch_failed", ticker=ticker, error=str(e))
+            # Try to use stale cache data if available (within 2 hours)
+            stale_cache_used = False
+            if uw_cache:
+                current_time = time.time()
+                stale_threshold = 2 * 3600  # 2 hours
                 
-                gex_map[ticker] = {"gamma_regime": "unknown"}
-                dp_map[ticker] = []
-                vol_map[ticker] = {"realized_vol_20d": 0}
-                ovl_map[ticker] = []
+                for ticker in Config.TICKERS:
+                    cache_data = uw_cache.get(ticker, {})
+                    if cache_data and not cache_data.get("simulated"):
+                        last_update = cache_data.get("_last_update", 0)
+                        age_sec = current_time - last_update if last_update else float('inf')
+                        
+                        # Use stale data if less than 2 hours old
+                        if age_sec < stale_threshold:
+                            stale_cache_used = True
+                            print(f"Γ£à Using stale cache for {ticker} (age: {int(age_sec/60)} min)", flush=True)
+                            
+                            # Extract flow trades from stale cache
+                            flow_trades_raw = cache_data.get("flow_trades", [])
+                            if flow_trades_raw:
+                                uw_client = UWClient()
+                                for raw_trade in flow_trades_raw:
+                                    try:
+                                        normalized_trade = uw_client._normalize_flow_trade(raw_trade)
+                                        if base_filter(normalized_trade):
+                                            all_trades.append(normalized_trade)
+                                    except Exception as e:
+                                        continue
+                            
+                            # Use cached sentiment/conviction data
+                            dp_data = cache_data.get("dark_pool", {})
+                            dp_map[ticker] = [{"off_lit_volume": dp_data.get("total_premium", 0)}]
+                            net_map[ticker] = {
+                                "net_premium": cache_data.get("net_premium", 0),
+                                "net_call_premium": cache_data.get("call_premium", 0)
+                            }
+                            sentiment = cache_data.get("sentiment", "NEUTRAL")
+                            gex_map[ticker] = {"gamma_regime": "negative" if sentiment == "BEARISH" else "neutral"}
+                            vol_map[ticker] = {"realized_vol_20d": 0.2}
+                            ovl_map[ticker] = []
+                
+                if stale_cache_used:
+                    print("Γ£à Using stale cache data (graceful degradation mode)", flush=True)
+                    log_event("uw_cache", "using_stale_cache", 
+                             action="graceful_degradation",
+                             note="Using cached data < 2 hours old due to API rate limit or daemon unavailable")
+                else:
+                    print("ΓÜá∩╕Å  No usable stale cache - skipping trading this cycle", flush=True)
+                    log_event("uw_cache", "cache_empty_no_stale", 
+                             action="skipping_trading",
+                             reason="no_cache_data_available")
             
-            if poll_flow: _smart_poller.record_success("option_flow")
+            # If no stale cache available, skip trading
+            if not stale_cache_used:
+                print("ΓÜá∩╕Å  Skipping API calls to preserve quota - no usable cache data", flush=True)
+                poll_top_net = False
+                poll_flow = False
+                
+                # Initialize empty maps (will result in no clusters)
+                for ticker in Config.TICKERS:
+                    gex_map[ticker] = {"gamma_regime": "unknown"}
+                    dp_map[ticker] = []
+                    vol_map[ticker] = {"realized_vol_20d": 0}
+                    ovl_map[ticker] = []
+                    net_map[ticker] = {}
 
         audit_seg("run_once", "data_fetch_complete")
         print(f"DEBUG: Fetched data, clustering {len(all_trades)} trades", flush=True)
         clusters = cluster_signals(all_trades)
         
         print(f"DEBUG: Initial clusters={len(clusters)}, use_composite={use_composite}", flush=True)
-        if use_composite:
+        
+        # CRITICAL FIX: Always run composite scoring when cache exists, even if flow_trades is empty
+        # Composite scoring uses sentiment, conviction, dark_pool, insider - doesn't need flow_trades
+        if use_composite and len(uw_cache) > 0:
             # Generate synthetic signals from cache instead of waiting for live API
+            # CRITICAL: This works even when flow_trades is empty - uses sentiment, conviction, dark_pool, insider
+            print(f"DEBUG: Running composite scoring for {len(uw_cache)} symbols (flow_trades may be empty)", flush=True)
             market_regime = compute_market_regime(gex_map, net_map, vol_map)
             filtered_clusters = []
             
+            symbols_processed = 0
+            symbols_with_signals = 0
+            
             for ticker in uw_cache.keys():
                 # Skip metadata keys
                 if ticker.startswith("_"):
@@ -4096,9 +5054,70 @@ def run_once():
                 
                 # V3: Enrichment ΓåÆ Composite V3 FULL INTELLIGENCE ΓåÆ Gate
                 enriched = uw_enrich.enrich_signal(ticker, uw_cache, market_regime)
+                
+                # CRITICAL FIX: If using stale cache, don't penalize freshness too much
+                # Freshness decay is already applied in enrichment, but for stale cache (< 2 hours),
+                # we should use a minimum freshness to allow trading
+                symbol_data = uw_cache.get(ticker, {})
+                if isinstance(symbol_data, dict):
+                    last_update = symbol_data.get("_last_update", 0)
+                    if last_update:
+                        age_sec = time.time() - last_update
+                        age_min = age_sec / 60.0
+                        # If cache is < 2 hours old (graceful degradation threshold), use minimum 0.7 freshness
+                        # This prevents freshness from killing all scores when using stale cache
+                        if age_min < 120:  # 2 hours
+                            current_freshness = enriched.get("freshness", 1.0)
+                            if current_freshness < 0.7:
+                                enriched["freshness"] = 0.7  # Set to minimum 0.7 for stale cache
+                                print(f"DEBUG: Adjusted freshness for {ticker} from {current_freshness:.2f} to 0.70 (stale cache < 2h)", flush=True)
+                
+                # Ensure computed signals are in enriched data (fallback if not in cache)
+                enricher = uw_enrich.UWEnricher()
+                cache_updated = False
+                
+                if isinstance(symbol_data, dict):
+                    # Compute missing signals on-the-fly
+                    if not enriched.get("iv_term_skew") and symbol_data.get("iv_term_skew") is None:
+                        computed_skew = enricher.compute_iv_term_skew(ticker, symbol_data)
+                        enriched["iv_term_skew"] = computed_skew
+                        if ticker in uw_cache:
+                            uw_cache[ticker]["iv_term_skew"] = computed_skew
+                            cache_updated = True
+                    
+                    if not enriched.get("smile_slope") and symbol_data.get("smile_slope") is None:
+                        computed_slope = enricher.compute_smile_slope(ticker, symbol_data)
+                        enriched["smile_slope"] = computed_slope
+                        if ticker in uw_cache:
+                            uw_cache[ticker]["smile_slope"] = computed_slope
+                            cache_updated = True
+                
+                # Ensure insider exists (with default structure)
+                if not enriched.get("insider") or not isinstance(enriched.get("insider"), dict):
+                    default_insider = {
+                        "sentiment": "NEUTRAL",
+                        "net_buys": 0,
+                        "net_sells": 0,
+                        "total_usd": 0.0,
+                        "conviction_modifier": 0.0
+                    }
+                    enriched["insider"] = symbol_data.get("insider", default_insider) if isinstance(symbol_data, dict) else default_insider
+                    if ticker in uw_cache and not uw_cache[ticker].get("insider"):
+                        uw_cache[ticker]["insider"] = enriched["insider"]
+                        cache_updated = True
+                
+                # Persist cache updates if any were made
+                if cache_updated:
+                    try:
+                        atomic_write_json(CacheFiles.UW_FLOW_CACHE, uw_cache)
+                    except Exception as e:
+                        log_event("cache_update", "error", error=str(e))
+                
                 # Use V3 scoring with all expanded intelligence (congress, shorts, institutional, etc.)
+                symbols_processed += 1
                 composite = uw_v2.compute_composite_score_v3(ticker, enriched, market_regime)
                 if composite is None:
+                    print(f"DEBUG: Composite scoring returned None for {ticker} - skipping", flush=True)
                     continue  # skip invalid data safely
                 
                 # V3: Log all expanded features for learning (congress, shorts, institutional, etc.)
@@ -4119,7 +5138,7 @@ def run_once():
                 
                 # V3 Attribution: Store enriched composite with FULL INTELLIGENCE features for learning
                 try:
-                    with open("data/uw_attribution.jsonl", "a") as f:
+                    with open(CacheFiles.UW_ATTRIBUTION, "a") as f:
                         attr_rec = {
                             "ts": int(time.time()),
                             "symbol": ticker,
@@ -4140,14 +5159,20 @@ def run_once():
                     pass  # Don't crash trading on attribution logging errors
                 
                 if gate_result:
+                    symbols_with_signals += 1
                     # Create synthetic cluster from cache data
                     # V3: Get sentiment from enriched data, include expanded intel
-                    flow_sentiment = enriched.get("sentiment", "NEUTRAL")
+                    flow_sentiment_raw = enriched.get("sentiment", "NEUTRAL")
+                    # CRITICAL FIX: Convert BULLISH/BEARISH to lowercase bullish/bearish for direction field
+                    # The code expects lowercase (see line 3908: side = "buy" if c["direction"] == "bullish")
+                    flow_sentiment = flow_sentiment_raw.lower() if flow_sentiment_raw in ("BULLISH", "BEARISH") else "neutral"
+                    score = composite.get("score", 0.0)
+                    print(f"DEBUG: Composite signal ACCEPTED for {ticker}: score={score:.2f}, sentiment={flow_sentiment_raw}->{flow_sentiment}, threshold={get_threshold(ticker, 'base'):.2f}", flush=True)
                     cluster = {
                         "ticker": ticker,
-                        "direction": flow_sentiment,  # Required for decision mapping
-                        "sentiment": flow_sentiment,
-                        "composite_score": composite.get("score", 0.0),
+                        "direction": flow_sentiment,  # CRITICAL: Must be lowercase "bullish"/"bearish"
+                        "sentiment": flow_sentiment_raw,  # Keep original for display
+                        "composite_score": score,
                         "composite_meta": composite,
                         "gate_passed": True,
                         "source": "composite_v3",
@@ -4160,13 +5185,35 @@ def run_once():
                     }
                     filtered_clusters.append(cluster)
                 else:
+                    score = composite.get("score", 0.0)
+                    threshold_used = get_threshold(ticker, 'base')
+                    toxicity = composite.get("toxicity", 0.0)
+                    freshness = composite.get("freshness", 1.0)
+                    
+                    # Determine actual rejection reason
+                    rejection_reasons = []
+                    if score < threshold_used:
+                        rejection_reasons.append(f"score={score:.2f} < threshold={threshold_used:.2f}")
+                    if toxicity > 0.90:
+                        rejection_reasons.append(f"toxicity={toxicity:.2f} > 0.90")
+                    if freshness < 0.30:
+                        rejection_reasons.append(f"freshness={freshness:.2f} < 0.30")
+                    
+                    reason_str = " OR ".join(rejection_reasons) if rejection_reasons else "unknown"
+                    print(f"DEBUG: Composite signal REJECTED for {ticker}: {reason_str}", flush=True)
                     log_event("composite_gate", "rejected", symbol=ticker, 
-                             score=composite.get("score", 0.0),
-                             threshold=adaptive_gate.state.get('threshold', 2.5))
+                             score=score,
+                             threshold=threshold_used,
+                             toxicity=toxicity,
+                             freshness=freshness,
+                             rejection_reason=reason_str)
             
             clusters = filtered_clusters
+            print(f"DEBUG: Composite scoring complete: {symbols_processed} symbols processed, {symbols_with_signals} passed gate, {len(clusters)} clusters generated", flush=True)
             log_event("composite_filter", "applied", cache_symbols=len(uw_cache), 
-                     passed=len(clusters), rejection_rate=1.0 - (len(clusters) / max(len(uw_cache), 1)))
+                     symbols_processed=symbols_processed,
+                     symbols_with_signals=symbols_with_signals,
+                     passed=len(clusters), rejection_rate=1.0 - (len(clusters) / max(symbols_processed, 1)))
             print(f"DEBUG: Composite filter complete, {len(clusters)} clusters passed gate", flush=True)
 
         audit_seg("run_once", "clusters_filtered", {"cluster_count": len(clusters)})
@@ -4196,12 +5243,34 @@ def run_once():
         _last_market_regime = market_regime
         
         print(f"DEBUG: About to call decide_and_execute with {len(clusters)} clusters, regime={market_regime}", flush=True)
+        if len(clusters) == 0:
+            print("ΓÜá∩╕Å  WARNING: No clusters to execute - check composite scoring logs above", flush=True)
+            log_event("execution", "no_clusters", cache_symbols=len(uw_cache) if use_composite else 0)
         audit_seg("run_once", "before_decide_execute", {"cluster_count": len(clusters)})
+        # Live-safety gates before placing NEW entries:
+        # - Broker degraded => reduce-only
+        # - Not armed / endpoint mismatch => skip entries
+        # - Executor not reconciled => skip entries (until it can sync positions cleanly)
+        armed = trading_is_armed()
+        reconciled_ok = False
+        try:
+            reconciled_ok = bool(engine.executor.ensure_reconciled())
+        except Exception:
+            reconciled_ok = False
+
         if degraded_mode:
             # Reduce-only safety: do not open new positions when broker connectivity is degraded.
             # Still allow exit logic and monitoring to run.
             log_event("run_once", "reduce_only_broker_degraded", action="skip_entries")
             orders = []
+        elif not armed:
+            log_event("run_once", "not_armed_skip_entries",
+                      trading_mode=Config.TRADING_MODE, base_url=Config.ALPACA_BASE_URL,
+                      require_live_ack=Config.REQUIRE_LIVE_ACK)
+            orders = []
+        elif not reconciled_ok:
+            log_event("run_once", "not_reconciled_skip_entries", action="skip_entries")
+            orders = []
         else:
             if Config.ENABLE_PER_TICKER_LEARNING:
                 decisions_map = build_symbol_decisions(clusters, gex_map, dp_map, net_map, vol_map, ovl_map)
@@ -4219,6 +5288,27 @@ def run_once():
         metrics["market_regime"] = market_regime
         metrics["composite_enabled"] = use_composite
         
+        # RISK MANAGEMENT: Add risk metrics to cycle metrics
+        try:
+            from risk_management import calculate_daily_pnl, load_peak_equity, get_risk_limits
+            account = engine.executor.api.get_account()
+            current_equity = float(account.equity)
+            daily_pnl = calculate_daily_pnl(current_equity)
+            peak_equity = load_peak_equity()
+            drawdown_pct = (peak_equity - current_equity) / peak_equity if peak_equity > 0 else 0
+            
+            metrics["risk_metrics"] = {
+                "current_equity": current_equity,
+                "peak_equity": peak_equity,
+                "daily_pnl": daily_pnl,
+                "drawdown_pct": drawdown_pct,
+                "daily_loss_limit": get_risk_limits()["daily_loss_dollar"],
+                "drawdown_limit_pct": get_risk_limits()["max_drawdown_pct"],
+                "mode": "PAPER" if Config.TRADING_MODE == "PAPER" else "LIVE"
+            }
+        except Exception:
+            pass  # Non-critical
+        
         print("DEBUG: About to log telemetry", flush=True)
         audit_seg("run_once", "before_telemetry")
         try:
@@ -4241,6 +5331,33 @@ def run_once():
         except Exception:
             pass
         
+        # CRITICAL FIX: Write heartbeat BEFORE owner_health_check to prevent false stale alerts
+        # heartbeat() is normally called after run_once() completes, but owner_health_check
+        # runs at the end of run_once() and checks heartbeat file - need to write it first
+        try:
+            watchdog.heartbeat({"clusters": len(clusters), "orders": len(orders)})
+        except Exception as e:
+            log_event("heartbeat", "early_write_failed", error=str(e))
+        
+        # CRITICAL FIX: Write heartbeat BEFORE owner_health_check to prevent false stale alerts
+        # The heartbeat file is checked by owner_health_check, but heartbeat() is normally
+        # called AFTER run_once() completes. We need to write it here so the check sees fresh data.
+        # Note: This is a duplicate write (heartbeat() also called after run_once), but ensures
+        # owner_health_check sees fresh heartbeat file.
+        try:
+            heartbeat_path = StateFiles.BOT_HEARTBEAT
+            heartbeat_path.parent.mkdir(parents=True, exist_ok=True)
+            heartbeat_data = {
+                "last_heartbeat_ts": int(time.time()),
+                "last_heartbeat_dt": datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S UTC"),
+                "iter_count": 0,  # Will be updated by actual heartbeat() call
+                "running": True,
+                "metrics": {"clusters": len(clusters), "orders": len(orders)}
+            }
+            heartbeat_path.write_text(json.dumps(heartbeat_data, indent=2))
+        except Exception as e:
+            log_event("heartbeat", "early_write_failed", error=str(e))
+        
         print("DEBUG: About to call owner_health_check", flush=True)
         audit_seg("run_once", "before_health_check")
         # Owner-in-the-loop health check cycle
@@ -4261,8 +5378,15 @@ def run_once():
         else:
             ZERO_ORDER_CYCLE_COUNT = 0
         
+        # Calculate average composite score from ALL symbols processed (not just clusters)
+        # This gives better visibility into why signals are being rejected
         composite_scores = [c.get("composite_score", 0.0) for c in clusters if c.get("source") == "composite"]
-        avg_score = sum(composite_scores) / len(composite_scores) if composite_scores else 5.0
+        if composite_scores:
+            avg_score = sum(composite_scores) / len(composite_scores)
+        else:
+            # If no clusters passed, use a default that won't trigger rollback
+            # (signals were rejected, not that scoring is broken)
+            avg_score = 3.0  # Changed from 5.0 to 3.0 to reflect actual rejection threshold
         
         rollback = check_rollback_conditions(
             composite_scores_avg=avg_score,
@@ -4286,7 +5410,7 @@ def run_once():
         
         # Collect cycle metrics for optimization engine
         exec_quality_data = []
-        exec_log_path = Path("data/execution_quality.jsonl")
+        exec_log_path = CacheFiles.EXECUTION_QUALITY
         if exec_log_path.exists():
             with exec_log_path.open("r") as f:
                 for line in f:
@@ -4374,7 +5498,20 @@ def daily_and_weekly_tasks_if_needed():
             log_event("daily", "uw_weight_tuner_failed", error=str(e))
         
         if Config.ENABLE_PER_TICKER_LEARNING:
+            # MEDIUM-TERM LEARNING: Daily batch processing
             learn_from_outcomes()
+            
+            # PROFITABILITY TRACKING: Update daily/weekly/monthly metrics
+            try:
+                from profitability_tracker import update_daily_performance, update_weekly_performance, update_monthly_performance
+                update_daily_performance()
+                # Update weekly on Fridays, monthly on first day of month
+                if is_friday():
+                    update_weekly_performance()
+                if datetime.now(timezone.utc).day == 1:
+                    update_monthly_performance()
+            except Exception as e:
+                log_event("profitability_tracking", "update_failed", error=str(e))
 
     if is_friday() and is_after_close_now():
         if _last_weekly_adjust_day != day:
@@ -4421,12 +5558,12 @@ class WorkerState:
         self.backoff_sec = Config.BACKOFF_BASE_SEC
         self.last_metrics = {}
         self.running = False
-        self.fail_counter_path = Path("state/fail_counter.json")
+        self.fail_counter_path = StateFiles.FAIL_COUNTER
     
     def _load_fail_count(self) -> int:
         """Load persistent fail counter from disk."""
         try:
-            fail_counter_path = Path("state/fail_counter.json")
+            fail_counter_path = StateFiles.FAIL_COUNTER
             if fail_counter_path.exists():
                 data = json.loads(fail_counter_path.read_text())
                 return int(data.get("fail_count", 0))
@@ -4452,6 +5589,38 @@ class Watchdog:
         if metrics:
             self.state.last_metrics = metrics
         log_event("heartbeat", "worker_alive", metrics=metrics or {})
+        
+        # CRITICAL FIX: Write heartbeat file so owner_health_check can find it
+        heartbeat_path = StateFiles.BOT_HEARTBEAT
+        try:
+            # Ensure directory exists
+            heartbeat_path.parent.mkdir(parents=True, exist_ok=True)
+            
+            heartbeat_data = {
+                "last_heartbeat_ts": int(self.state.last_heartbeat),
+                "last_heartbeat_dt": datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S UTC"),
+                "iter_count": self.state.iter_count,
+                "running": self.state.running,
+                "metrics": metrics or {}
+            }
+            
+            # Write file - use simple write_text (same as owner_health_check)
+            heartbeat_path.write_text(json.dumps(heartbeat_data, indent=2))
+            
+            # Verify file was written
+            if not heartbeat_path.exists():
+                print(f"ERROR: Heartbeat file write failed - file doesn't exist: {heartbeat_path}", flush=True)
+                log_event("heartbeat", "write_verify_failed", path=str(heartbeat_path))
+            else:
+                # Success - log occasionally (not every heartbeat to avoid spam)
+                if self.state.iter_count % 10 == 0:
+                    print(f"DEBUG: Heartbeat file OK: {heartbeat_path} (iter {self.state.iter_count})", flush=True)
+                    
+        except Exception as e:
+            # CRITICAL: Log the error so we can see why it's failing
+            print(f"ERROR: Failed to write heartbeat file to {heartbeat_path}: {e}", flush=True)
+            import traceback
+            log_event("heartbeat", "write_failed", error=str(e), path=str(heartbeat_path), traceback=traceback.format_exc())
 
     def _worker_loop(self):
         self.state.running = True
@@ -4465,10 +5634,24 @@ class Watchdog:
             try:
                 log_event("worker", "iter_start", iter=self.state.iter_count + 1)
                 
-                if is_market_open_now() or SIMULATE_MARKET_OPEN:
+                market_open = is_market_open_now() or SIMULATE_MARKET_OPEN
+                
+                if market_open:
                     metrics = run_once()
                 else:
-                    metrics = {"market_open": False}
+                    # Market closed - still log cycle but skip trading
+                    metrics = {"market_open": False, "clusters": 0, "orders": 0}
+                    # CRITICAL: Always log cycles to run.jsonl for visibility
+                    jsonl_write("run", {
+                        "ts": int(time.time()),
+                        "_ts": int(time.time()),
+                        "msg": "cycle_complete",
+                        "clusters": 0,
+                        "orders": 0,
+                        "market_open": False,
+                        "metrics": metrics
+                    })
+                    log_event("run", "complete", clusters=0, orders=0, metrics=metrics, market_open=False)
                 
                 daily_and_weekly_tasks_if_needed()
                 self.state.iter_count += 1
@@ -4477,7 +5660,7 @@ class Watchdog:
                 self.state.backoff_sec = Config.BACKOFF_BASE_SEC
                 self.heartbeat(metrics)
                 
-                log_event("worker", "iter_end", iter=self.state.iter_count, success=True)
+                log_event("worker", "iter_end", iter=self.state.iter_count, success=True, market_open=market_open)
                 
             except Exception as e:
                 self.state.fail_count += 1
@@ -4491,7 +5674,7 @@ class Watchdog:
                 send_webhook({"event": "iteration_failed", "error": str(e), "fail_count": self.state.fail_count})
                 
                 if self.state.fail_count >= 5:
-                    freeze_path = Path("state/pre_market_freeze.flag")
+                    freeze_path = StateFiles.PRE_MARKET_FREEZE
                     freeze_path.write_text("too_many_failures")
                     log_event("worker_error", "freeze_activated", reason="too_many_failures", fail_count=self.state.fail_count)
                     self.state.backoff_sec = 300
@@ -4541,6 +5724,166 @@ class Watchdog:
 app = Flask(__name__)
 watchdog = Watchdog()
 
+# Self-healing monitor thread
+_self_healing_last_run = 0
+_self_healing_interval = 300  # Run every 5 minutes
+
+def run_self_healing_periodic():
+    """Periodically run self-healing monitor."""
+    global _self_healing_last_run
+    while True:
+        try:
+            time.sleep(60)  # Check every minute
+            now = time.time()
+            
+            # Run healing every 5 minutes
+            if now - _self_healing_last_run >= _self_healing_interval:
+                try:
+                    from self_healing_monitor import SelfHealingMonitor
+                    monitor = SelfHealingMonitor()
+                    result = monitor.run_healing_cycle()
+                    _self_healing_last_run = now
+                    log_event("self_healing", "cycle_complete", 
+                             issues_detected=result.get("issues_detected", 0),
+                             issues_healed=result.get("issues_healed", 0))
+                except ImportError:
+                    # Self-healing not available, skip
+                    pass
+                except Exception as e:
+                    log_event("self_healing", "error", error=str(e))
+        except Exception as e:
+            log_event("self_healing", "thread_error", error=str(e))
+            time.sleep(60)
+
+# Start self-healing thread
+if __name__ == "__main__":
+    healing_thread = threading.Thread(target=run_self_healing_periodic, daemon=True, name="SelfHealingMonitor")
+    healing_thread.start()
+    
+    # Start cache enrichment service
+    def run_cache_enrichment_periodic():
+        """Periodically enrich cache with computed signals."""
+        # Run immediately on startup
+        try:
+            from cache_enrichment_service import CacheEnrichmentService
+            service = CacheEnrichmentService()
+            service.run_once()
+            log_event("cache_enrichment", "startup_enrichment_complete")
+        except ImportError:
+            # Service not available, skip
+            pass
+        except Exception as e:
+            log_event("cache_enrichment", "startup_error", error=str(e))
+        
+        # Then run every 60 seconds
+        while True:
+            try:
+                time.sleep(60)  # Check every minute
+                try:
+                    from cache_enrichment_service import CacheEnrichmentService
+                    service = CacheEnrichmentService()
+                    service.run_once()
+                    log_event("cache_enrichment", "cycle_complete")
+                except ImportError:
+                    # Service not available, skip
+                    pass
+                except Exception as e:
+                    log_event("cache_enrichment", "error", error=str(e))
+            except Exception as e:
+                log_event("cache_enrichment", "thread_error", error=str(e))
+                time.sleep(60)
+    
+    cache_enrichment_thread = threading.Thread(target=run_cache_enrichment_periodic, daemon=True, name="CacheEnrichmentService")
+    cache_enrichment_thread.start()
+    
+    # Start comprehensive learning orchestrator (runs daily after market close)
+    def run_comprehensive_learning_periodic():
+        """
+        Run comprehensive learning on multiple schedules:
+        - Daily: After market close
+        - Weekly: Every Friday after market close
+        - Bi-Weekly: Every other Friday after market close
+        - Monthly: First trading day of month after market close
+        """
+        last_run_date = None
+        
+        while True:
+            try:
+                # Check if we should run today (after market close, once per day)
+                today = datetime.now(timezone.utc).date()
+                
+                # Use existing market close detection (handles DST properly)
+                market_closed = is_after_close_now()
+                
+                # Run if: (1) market is closed, (2) we haven't run today yet
+                should_run_daily = False
+                if market_closed and last_run_date != today:
+                    should_run_daily = True
+                    log_event("comprehensive_learning", "scheduled_run_triggered", reason="market_closed", date=str(today))
+                
+                if should_run_daily:
+                    try:
+                        # Use NEW comprehensive learning orchestrator V2
+                        from comprehensive_learning_orchestrator_v2 import run_daily_learning
+                        results = run_daily_learning()
+                        last_run_date = today
+                        
+                        # Log results
+                        log_event("comprehensive_learning", "daily_cycle_complete",
+                                 attribution=results.get("attribution", 0),
+                                 exits=results.get("exits", 0),
+                                 signals=results.get("signals", 0),
+                                 orders=results.get("orders", 0),
+                                 blocked_trades=results.get("blocked_trades", 0),
+                                 gate_events=results.get("gate_events", 0),
+                                 uw_blocked=results.get("uw_blocked", 0),
+                                 weights_updated=results.get("weights_updated", 0))
+                        
+                        # Force weight cache refresh in trading engine
+                        # This ensures updated weights are immediately available
+                        try:
+                            import uw_composite_v2
+                            # Invalidate cache by setting timestamp to 0
+                            # This forces reload on next get_weight() call
+                            uw_composite_v2._weights_cache_ts = 0.0
+                            uw_composite_v2._cached_weights.clear()
+                            # Also invalidate multiplier cache
+                            uw_composite_v2._multipliers_cache_ts = 0.0
+                            uw_composite_v2._cached_multipliers.clear()
+                            log_event("comprehensive_learning", "weight_cache_refreshed")
+                        except Exception as e:
+                            log_event("comprehensive_learning", "cache_refresh_warning", error=str(e))
+                            
+                    except ImportError:
+                        # Service not available, skip
+                        pass
+                    except Exception as e:
+                        log_event("comprehensive_learning", "error", error=str(e))
+                        import traceback
+                        log_event("comprehensive_learning", "error_traceback", traceback=traceback.format_exc())
+                
+                # Check for weekly/bi-weekly/monthly cycles
+                try:
+                    from comprehensive_learning_scheduler import check_and_run_scheduled_cycles
+                    scheduled_results = check_and_run_scheduled_cycles()
+                    if scheduled_results:
+                        log_event("comprehensive_learning", "scheduled_cycles_executed", cycles=list(scheduled_results.keys()))
+                except ImportError:
+                    pass  # Scheduler not available
+                except Exception as e:
+                    log_event("comprehensive_learning", "scheduler_error", error=str(e))
+                
+                # Sleep for 1 hour, then check again
+                # This is safe because we only run once per day (checked by last_run_date)
+                time.sleep(3600)
+                
+            except Exception as e:
+                log_event("comprehensive_learning", "thread_error", error=str(e))
+                time.sleep(3600)  # Retry after 1 hour on error
+    
+    comprehensive_learning_thread = threading.Thread(target=run_comprehensive_learning_periodic, daemon=True, name="ComprehensiveLearning")
+    comprehensive_learning_thread.start()
+
 @app.route("/", methods=["GET"])
 def root():
     return jsonify({"status": "ok", "service": "trading-bot"}), 200
@@ -4561,6 +5904,47 @@ def health():
     except Exception as e:
         status["health_checks_error"] = str(e)
     
+    # Add SRE monitoring data
+    try:
+        from sre_monitoring import get_sre_health
+        sre_health = get_sre_health()
+        status["sre_health"] = {
+            "market_open": sre_health.get("market_open", False),
+            "market_status": sre_health.get("market_status", "unknown"),
+            "last_order": sre_health.get("last_order", {}),
+            "overall_health": sre_health.get("overall_health", "unknown"),
+            "uw_api_healthy_count": sum(1 for h in sre_health.get("uw_api_endpoints", {}).values() if h.get("status") == "healthy"),
+            "uw_api_total_count": len(sre_health.get("uw_api_endpoints", {})),
+            "signal_components_healthy": sum(1 for s in sre_health.get("signal_components", {}).values() if s.get("status") == "healthy"),
+            "signal_components_total": len(sre_health.get("signal_components", {}))
+        }
+    except Exception as e:
+        status["sre_health_error"] = str(e)
+    
+    # Add comprehensive learning health (v2)
+    try:
+        from comprehensive_learning_orchestrator_v2 import load_learning_state
+        state = load_learning_state()
+        last_processed = state.get("last_processed_ts")
+        if last_processed:
+            try:
+                last_dt = datetime.fromisoformat(last_processed.replace("Z", "+00:00"))
+                age_sec = (datetime.now(timezone.utc) - last_dt).total_seconds()
+            except:
+                age_sec = None
+        else:
+            age_sec = None
+        
+        status["comprehensive_learning"] = {
+            "status": "active",
+            "last_run_age_sec": age_sec,
+            "total_trades_processed": state.get("total_trades_processed", 0),
+            "total_trades_learned_from": state.get("total_trades_learned_from", 0),
+            "note": "Using comprehensive_learning_orchestrator_v2"
+        }
+    except Exception as e:
+        status["comprehensive_learning_error"] = str(e)
+    
     return jsonify(status), 200
 
 @app.route("/metrics", methods=["GET"])
@@ -4927,12 +6311,29 @@ def api_cockpit():
         except Exception:
             pass
         
+        # Get accurate last order timestamp
+        last_order_ts = None
+        last_order_age_sec = None
+        try:
+            from sre_monitoring import SREMonitoringEngine
+            engine = SREMonitoringEngine()
+            last_order_ts = engine.get_last_order_timestamp()
+            if last_order_ts:
+                last_order_age_sec = time.time() - last_order_ts
+        except Exception:
+            pass
+        
         return jsonify({
             "mode": trading_mode.get("mode", "PAPER"),
             "capital_ramp": capital_ramp,
             "kpis": {"win_rate": win_rate, "total_trades": total_trades, "status": "ok"},
             "positions": positions,
             "uw": {"primary_watchlist": Config.TICKERS, "flow": uw_cache},
+            "last_order": {
+                "timestamp": last_order_ts,
+                "age_sec": last_order_age_sec,
+                "age_hours": last_order_age_sec / 3600 if last_order_age_sec else None
+            },
             "last_update": int(time.time())
         }), 200
     except Exception as e:
@@ -5008,6 +6409,69 @@ def dashboard_incidents():
         "health_check": health_check_passes()
     }), 200
 
+@app.route("/api/sre/health", methods=["GET"])
+def api_sre_health():
+    """SRE-style comprehensive health monitoring endpoint"""
+    try:
+        # Trigger cache enrichment before checking to ensure signals are present
+        try:
+            from cache_enrichment_service import CacheEnrichmentService
+            service = CacheEnrichmentService()
+            service.run_once()
+        except Exception:
+            # Continue even if enrichment fails
+            pass
+        
+        from sre_monitoring import get_sre_health
+        health = get_sre_health()
+        return jsonify(health), 200
+    except Exception as e:
+        return jsonify({"error": str(e)}), 500
+
+@app.route("/api/sre/signals", methods=["GET"])
+def api_sre_signals():
+    """Get detailed signal component health"""
+    try:
+        from sre_monitoring import SREMonitoringEngine
+        engine = SREMonitoringEngine()
+        signals = engine.check_signal_generation_health()
+        return jsonify({
+            "signals": {
+                name: {
+                    "status": s.status,
+                    "last_update_age_sec": s.last_update_age_sec,
+                    "data_freshness_sec": s.data_freshness_sec,
+                    "error_rate_1h": s.error_rate_1h,
+                    "details": s.details
+                }
+                for name, s in signals.items()
+            }
+        }), 200
+    except Exception as e:
+        return jsonify({"error": str(e)}), 500
+
+@app.route("/api/sre/uw_endpoints", methods=["GET"])
+def api_sre_uw_endpoints():
+    """Get UW API endpoint health"""
+    try:
+        from sre_monitoring import SREMonitoringEngine
+        engine = SREMonitoringEngine()
+        endpoints = engine.check_uw_api_health()
+        return jsonify({
+            "endpoints": {
+                name: {
+                    "status": h.status,
+                    "error_rate_1h": h.error_rate_1h,
+                    "avg_latency_ms": h.avg_latency_ms,
+                    "rate_limit_remaining": h.rate_limit_remaining,
+                    "last_error": h.last_error
+                }
+                for name, h in endpoints.items()
+            }
+        }), 200
+    except Exception as e:
+        return jsonify({"error": str(e)}), 500
+
 @app.route("/debug/threads", methods=["GET"])
 def debug_threads():
     """Debug endpoint to check thread status"""
@@ -5038,8 +6502,15 @@ def handle_exit(signum, frame):
     finally:
         sys.exit(0)
 
-signal.signal(signal.SIGINT, handle_exit)
-signal.signal(signal.SIGTERM, handle_exit)
+# CRITICAL FIX: Only register signals when script is run directly (not when imported)
+# This prevents "signal only works in main thread" error when risk_management.py imports main.py
+if __name__ == "__main__":
+    try:
+        signal.signal(signal.SIGINT, handle_exit)
+        signal.signal(signal.SIGTERM, handle_exit)
+    except (ValueError, AttributeError):
+        # Signal registration failed (not in main thread) - safe to ignore when imported
+        pass
 
 # =========================
 # CONTINUOUS HEALTH CHECKS
@@ -5218,7 +6689,7 @@ def startup_reconcile_positions():
     Alpaca is source of truth. Halts trading if reconciliation fails.
     TIMEOUT PROTECTED: 10s max to prevent workflow startup hangs.
     """
-    reconcile_log_path = Path("logs/reconcile.jsonl")
+    reconcile_log_path = LogFiles.RECONCILE
     reconcile_log_path.parent.mkdir(exist_ok=True)
     
     try:
@@ -5238,7 +6709,7 @@ def startup_reconcile_positions():
         
         # Load bot's internal state with locking
         metadata_path = StateFiles.POSITION_METADATA
-        champions_path = Path("state/champions.json")
+        champions_path = StateFiles.CHAMPIONS
         
         local_metadata = load_metadata_with_lock(metadata_path)
         
@@ -5314,13 +6785,17 @@ def startup_reconcile_positions():
         
     except Exception as e:
         log_event("reconcile", "startup_failed", error=str(e))
-        send_webhook({
-            "event": "RECONCILE_FAILURE_HALT_TRADING",
-            "error": str(e),
-            "timestamp": datetime.utcnow().isoformat()
-        })
-        # DO NOT start trading if reconciliation fails
-        raise RuntimeError(f"Startup reconciliation failed - cannot verify position state: {e}")
+        # Don't send webhook on every startup failure (too noisy)
+        # send_webhook({
+        #     "event": "RECONCILE_FAILURE_HALT_TRADING",
+        #     "error": str(e),
+        #     "timestamp": datetime.utcnow().isoformat()
+        # })
+        # DO NOT raise - let main() handle it and continue
+        # The bot should start even if reconciliation fails (will retry in background)
+        print(f"WARNING: Startup reconciliation failed: {e}")
+        print("Bot will continue - reconciliation will retry in background")
+        return False  # Return False instead of raising
 
 # =========================
 # ENTRY POINT
@@ -5348,16 +6823,34 @@ def main():
         print("Flask server starting anyway to allow monitoring...")
         # DO NOT sys.exit(1) - allow server to start for health monitoring
     
-    watchdog.start()
-    supervisor = threading.Thread(target=watchdog.supervise, daemon=True)
-    supervisor.start()
+    # Start watchdog with error handling
+    try:
+        watchdog.start()
+        supervisor = threading.Thread(target=watchdog.supervise, daemon=True)
+        supervisor.start()
+        log_event("system", "watchdog_started")
+    except Exception as e:
+        log_event("system", "watchdog_start_failed", error=str(e))
+        print(f"WARNING: Watchdog failed to start: {e}")
+        import traceback
+        traceback.print_exc()
+        # Continue anyway - bot can run without watchdog
     
-    health_super = get_supervisor()
-    health_super.start()
-    log_event("system", "health_supervisor_started")
+    # Start health supervisor with error handling
+    try:
+        health_super = get_supervisor()
+        health_super.start()
+        log_event("system", "health_supervisor_started")
+    except Exception as e:
+        log_event("system", "health_supervisor_start_failed", error=str(e))
+        print(f"WARNING: Health supervisor failed to start: {e}")
+        print("Bot will continue without health supervisor...")
+        import traceback
+        traceback.print_exc()
     
     log_event("system", "api_start", port=Config.API_PORT)
-    app.run(host="0.0.0.0", port=Config.API_PORT)
+    print(f"Starting Flask server on port {Config.API_PORT}...", flush=True)
+    app.run(host="0.0.0.0", port=Config.API_PORT, debug=False)
 
 if __name__ == "__main__":
     # INVINCIBLE MAIN LOOP: Catch-all exception handler prevents process exit
diff --git a/signals/uw_adaptive.py b/signals/uw_adaptive.py
index a08b232..c87ee15 100644
--- a/signals/uw_adaptive.py
+++ b/signals/uw_adaptive.py
@@ -19,12 +19,14 @@ Files:
 - data/adaptive_gate_state.json
 """
 
+from config.registry import StateFiles, CacheFiles, LogFiles, ConfigFiles
 import json
 import time
 from pathlib import Path
 from typing import Dict, Any, Optional, Tuple, List
+from config.registry import StateFiles
 
-STATE_FILE = Path("data/adaptive_gate_state.json")
+STATE_FILE = StateFiles.ADAPTIVE_GATE_STATE
 
 # Default bucket edges for composite UW scores
 BUCKETS = [
diff --git a/uw_flow_daemon.py b/uw_flow_daemon.py
new file mode 100644
index 0000000..e85e75a
--- /dev/null
+++ b/uw_flow_daemon.py
@@ -0,0 +1,1324 @@
+#!/usr/bin/env python3
+"""
+UW Flow Daemon
+==============
+Continuously polls Unusual Whales API and populates uw_flow_cache.json.
+This is the ONLY component that should make UW API calls.
+
+Uses SmartPoller to optimize API usage based on data freshness requirements.
+"""
+
+import os
+import sys
+import time
+import json
+import signal
+import requests
+from pathlib import Path
+from datetime import datetime, timezone
+from typing import Dict, Any, List, Optional
+from dotenv import load_dotenv
+
+# Signal-safe print function to avoid reentrant call issues
+_print_lock = False
+def safe_print(*args, **kwargs):
+    """Print that's safe to call from signal handlers and avoids reentrant calls."""
+    global _print_lock
+    if _print_lock:
+        return  # Prevent reentrant calls
+    _print_lock = True
+    try:
+        msg = ' '.join(str(a) for a in args) + '\n'
+        os.write(1, msg.encode())  # stdout file descriptor is 1
+    except:
+        pass  # If print fails, just continue
+    finally:
+        _print_lock = False
+
+# #region agent log
+DEBUG_LOG_PATH = Path(__file__).parent / ".cursor" / "debug.log"
+_DEBUG_LOGGING = False  # Flag to prevent reentrant debug logging
+def debug_log(location, message, data=None, hypothesis_id=None):
+    global _DEBUG_LOGGING
+    if _DEBUG_LOGGING:
+        return  # Prevent reentrant calls
+    _DEBUG_LOGGING = True
+    try:
+        # Ensure directory exists
+        try:
+            DEBUG_LOG_PATH.parent.mkdir(parents=True, exist_ok=True)
+        except Exception as dir_err:
+            # If directory creation fails, try to write error to stderr
+            try:
+                os.write(2, f"[DEBUG-ERROR] Failed to create dir {DEBUG_LOG_PATH.parent}: {dir_err}\n".encode())
+            except:
+                pass
+            _DEBUG_LOGGING = False
+            return
+        
+        # Create log entry
+        try:
+            log_entry = json.dumps({
+                "sessionId": "uw-daemon-debug",
+                "runId": "run1",
+                "hypothesisId": hypothesis_id,
+                "location": location,
+                "message": message,
+                "data": data or {},
+                "timestamp": int(time.time() * 1000)
+            }) + "\n"
+        except Exception as json_err:
+            try:
+                os.write(2, f"[DEBUG-ERROR] Failed to create JSON: {json_err}\n".encode())
+            except:
+                pass
+            _DEBUG_LOGGING = False
+            return
+        
+        # Write to file
+        try:
+            with DEBUG_LOG_PATH.open("a") as f:
+                f.write(log_entry)
+                f.flush()  # Force flush to ensure it's written
+        except Exception as write_err:
+            try:
+                os.write(2, f"[DEBUG-ERROR] Failed to write to {DEBUG_LOG_PATH}: {write_err}\n".encode())
+            except:
+                pass
+            _DEBUG_LOGGING = False
+            return
+        
+        # Use os.write to avoid reentrant print issues (optional debug output to stderr)
+        try:
+            debug_msg = f"[DEBUG] {location}: {message} {json.dumps(data or {})}\n"
+            os.write(2, debug_msg.encode())  # Write directly to stderr file descriptor
+        except:
+            pass  # If stderr write fails, continue - file write succeeded
+    except Exception as e:
+        # Use os.write for error reporting too
+        try:
+            error_msg = f"[DEBUG-ERROR] Unexpected error in debug_log: {e}\n"
+            os.write(2, error_msg.encode())
+            import traceback
+            tb_msg = f"[DEBUG-ERROR] Traceback: {traceback.format_exc()}\n"
+            os.write(2, tb_msg.encode())
+        except:
+            pass
+    finally:
+        _DEBUG_LOGGING = False
+# #endregion
+
+# Add parent directory to path
+sys.path.insert(0, str(Path(__file__).parent))
+
+try:
+    from config.registry import CacheFiles, Directories, StateFiles, read_json, atomic_write_json, append_jsonl
+    # Test debug_log immediately after imports
+    try:
+        debug_log("uw_flow_daemon.py:imports", "Imports successful", {}, "H1")
+    except Exception as debug_err:
+        # If debug_log fails, write to stderr directly
+        try:
+            os.write(2, f"[CRITICAL] debug_log failed: {debug_err}\n".encode())
+        except:
+            pass
+except Exception as e:
+    try:
+        debug_log("uw_flow_daemon.py:imports", "Import failed", {"error": str(e)}, "H1")
+    except:
+        pass
+    raise
+
+load_dotenv()
+
+DATA_DIR = Directories.DATA
+CACHE_FILE = CacheFiles.UW_FLOW_CACHE
+
+class UWClient:
+    """Unusual Whales API client."""
+    
+    def __init__(self, api_key=None):
+        from config.registry import APIConfig
+        self.api_key = api_key or os.getenv("UW_API_KEY")
+        self.base = APIConfig.UW_BASE_URL
+        self.headers = {"Authorization": f"Bearer {self.api_key}"} if self.api_key else {}
+    
+    def _get(self, path_or_url: str, params: dict = None) -> dict:
+        """Make API request with quota tracking."""
+        url = path_or_url if path_or_url.startswith("http") else f"{self.base}{path_or_url}"
+        
+        # #region agent log
+        debug_log("uw_flow_daemon.py:_get", "API call attempt", {"url": url, "has_api_key": bool(self.api_key)}, "H3")
+        # #endregion
+        
+        # QUOTA TRACKING: Log all UW API calls
+        quota_log = CacheFiles.UW_API_QUOTA
+        quota_log.parent.mkdir(parents=True, exist_ok=True)
+        try:
+            with quota_log.open("a") as f:
+                f.write(json.dumps({
+                    "ts": int(time.time()),
+                    "url": url,
+                    "params": params or {},
+                    "source": "uw_flow_daemon"
+                }) + "\n")
+        except Exception:
+            pass  # Don't fail on quota logging
+        
+        try:
+            r = requests.get(url, headers=self.headers, params=params or {}, timeout=10)
+            
+            # Check rate limit headers
+            daily_count = r.headers.get("x-uw-daily-req-count")
+            daily_limit = r.headers.get("x-uw-token-req-limit")
+            
+            if daily_count and daily_limit:
+                count = int(daily_count)
+                limit = int(daily_limit)
+                pct = (count / limit * 100) if limit > 0 else 0
+                
+                # Log if we're getting close to limit
+                if pct > 75:
+                    safe_print(f"[UW-DAEMON] ΓÜá∩╕Å  Rate limit warning: {count}/{limit} ({pct:.1f}%)")
+                elif pct > 90:
+                    safe_print(f"[UW-DAEMON] ≡ƒÜ¿ Rate limit critical: {count}/{limit} ({pct:.1f}%)")
+            
+                # Check for 429 (rate limited)
+            if r.status_code == 429:
+                error_data = r.json() if r.content else {}
+                safe_print(f"[UW-DAEMON] Γ¥î RATE LIMITED (429): {error_data.get('message', 'Daily limit hit')}")
+                safe_print(f"[UW-DAEMON] ΓÜá∩╕Å  Stopping polling until limit resets (8PM EST)")
+                append_jsonl(CacheFiles.UW_FLOW_CACHE_LOG, {
+                    "event": "UW_API_RATE_LIMITED",
+                    "url": url,
+                    "status": 429,
+                    "daily_count": daily_count,
+                    "daily_limit": daily_limit,
+                    "message": error_data.get("message", ""),
+                    "ts": int(time.time())
+                })
+                # Set a flag to stop polling for a while
+                # The daemon will continue running but won't make API calls
+                return {"data": [], "_rate_limited": True}
+            
+            # Log non-200 responses for debugging
+            if r.status_code != 200:
+                safe_print(f"[UW-DAEMON] ΓÜá∩╕Å  API returned status {r.status_code} for {url}")
+                try:
+                    error_text = r.text[:200] if r.text else "No response body"
+                    safe_print(f"[UW-DAEMON] Response: {error_text}")
+                except:
+                    pass
+            
+            r.raise_for_status()
+            response_data = r.json()
+            # #region agent log
+            debug_log("uw_flow_daemon.py:_get", "API call success", {
+                "url": url, 
+                "status": r.status_code,
+                "has_data": bool(response_data.get("data")),
+                "data_type": type(response_data.get("data")).__name__,
+                "data_keys": list(response_data.keys()) if isinstance(response_data, dict) else []
+            }, "H3")
+            # #endregion
+            return response_data
+        except requests.exceptions.HTTPError as e:
+            # #region agent log
+            debug_log("uw_flow_daemon.py:_get", "API HTTP error", {
+                "url": url,
+                "status": getattr(e.response, 'status_code', None),
+                "error": str(e)
+            }, "H3")
+            # #endregion
+            append_jsonl(CacheFiles.UW_FLOW_CACHE_LOG, {
+                "event": "UW_API_ERROR",
+                "url": url,
+                "error": str(e),
+                "status_code": getattr(e.response, 'status_code', None),
+                "ts": int(time.time())
+            })
+            return {"data": []}
+        except Exception as e:
+            # #region agent log
+            debug_log("uw_flow_daemon.py:_get", "API exception", {
+                "url": url,
+                "error": str(e),
+                "error_type": type(e).__name__
+            }, "H3")
+            # #endregion
+            append_jsonl(CacheFiles.UW_FLOW_CACHE_LOG, {
+                "event": "UW_API_ERROR",
+                "url": url,
+                "error": str(e),
+                "ts": int(time.time())
+            })
+            return {"data": []}
+    
+    def get_option_flow(self, ticker: str, limit: int = 100) -> List[Dict]:
+        """Get option flow for a ticker."""
+        raw = self._get("/api/option-trades/flow-alerts", params={"symbol": ticker, "limit": limit})
+        data = raw.get("data", [])
+        if data:
+            safe_print(f"[UW-DAEMON] Retrieved {len(data)} flow trades for {ticker}")
+        return data
+    
+    def get_dark_pool_levels(self, ticker: str) -> List[Dict]:
+        """Get dark pool levels for a ticker."""
+        raw = self._get(f"/api/darkpool/{ticker}")
+        return raw.get("data", [])
+    
+    def get_greek_exposure(self, ticker: str) -> Dict:
+        """Get Greek exposure for a ticker (detailed exposure data)."""
+        # FIXED: Use correct endpoint per uw_signal_contracts.py
+        raw = self._get(f"/api/stock/{ticker}/greek-exposure")
+        data = raw.get("data", {})
+        if isinstance(data, list) and len(data) > 0:
+            data = data[0]
+        return data if isinstance(data, dict) else {}
+    
+    def get_greeks(self, ticker: str) -> Dict:
+        """Get Greeks for a ticker (basic greeks data - different from greek_exposure)."""
+        # This is a separate endpoint from greek_exposure (per sre_monitoring.py core_endpoints)
+        raw = self._get(f"/api/stock/{ticker}/greeks")
+        data = raw.get("data", {})
+        if isinstance(data, list) and len(data) > 0:
+            data = data[0]
+        return data if isinstance(data, dict) else {}
+    
+    def get_top_net_impact(self, limit: int = 50) -> List[Dict]:
+        """Get top net impact symbols."""
+        raw = self._get("/api/market/top-net-impact", params={"limit": limit})
+        return raw.get("data", [])
+    
+    def get_market_tide(self) -> Dict:
+        """Get market-wide options sentiment (market tide)."""
+        raw = self._get("/api/market/market-tide")
+        # #region agent log
+        debug_log("uw_flow_daemon.py:get_market_tide", "Raw API response", {
+            "raw_type": type(raw).__name__,
+            "raw_keys": list(raw.keys()) if isinstance(raw, dict) else [],
+            "has_data_key": "data" in raw if isinstance(raw, dict) else False
+        }, "H3")
+        # #endregion
+        
+        data = raw.get("data", {})
+        # #region agent log
+        debug_log("uw_flow_daemon.py:get_market_tide", "Extracted data", {
+            "data_type": type(data).__name__,
+            "is_list": isinstance(data, list),
+            "list_len": len(data) if isinstance(data, list) else 0,
+            "is_dict": isinstance(data, dict),
+            "dict_keys": list(data.keys()) if isinstance(data, dict) else []
+        }, "H3")
+        # #endregion
+        
+        if isinstance(data, list):
+            if len(data) > 0:
+                data = data[0]
+            else:
+                # Empty list - return empty dict
+                # #region agent log
+                debug_log("uw_flow_daemon.py:get_market_tide", "Empty list returned", {}, "H3")
+                # #endregion
+                return {}
+        
+        # If data is already a dict, return it; otherwise return empty dict
+        result = data if isinstance(data, dict) else {}
+        # #region agent log
+        debug_log("uw_flow_daemon.py:get_market_tide", "Final result", {
+            "result_type": type(result).__name__,
+            "result_keys": list(result.keys()) if isinstance(result, dict) else [],
+            "result_empty": not bool(result)
+        }, "H3")
+        # #endregion
+        return result
+    
+    def get_oi_change(self, ticker: str) -> Dict:
+        """Get open interest changes for a ticker."""
+        raw = self._get(f"/api/stock/{ticker}/oi-change")
+        data = raw.get("data", {})
+        if isinstance(data, list) and len(data) > 0:
+            data = data[0]
+        return data if isinstance(data, dict) else {}
+    
+    def get_etf_flow(self, ticker: str) -> Dict:
+        """Get ETF inflow/outflow for a ticker."""
+        raw = self._get(f"/api/etfs/{ticker}/in-outflow")
+        data = raw.get("data", {})
+        if isinstance(data, list) and len(data) > 0:
+            data = data[0]
+        return data if isinstance(data, dict) else {}
+    
+    def get_iv_rank(self, ticker: str) -> Dict:
+        """Get IV rank for a ticker."""
+        raw = self._get(f"/api/stock/{ticker}/iv-rank")
+        data = raw.get("data", {})
+        if isinstance(data, list) and len(data) > 0:
+            data = data[0]
+        return data if isinstance(data, dict) else {}
+    
+    def get_shorts_ftds(self, ticker: str) -> Dict:
+        """Get fails-to-deliver data for a ticker."""
+        raw = self._get(f"/api/shorts/{ticker}/ftds")
+        data = raw.get("data", {})
+        if isinstance(data, list) and len(data) > 0:
+            data = data[0]
+        return data if isinstance(data, dict) else {}
+    
+    def get_max_pain(self, ticker: str) -> Dict:
+        """Get max pain for a ticker."""
+        raw = self._get(f"/api/stock/{ticker}/max-pain")
+        data = raw.get("data", {})
+        if isinstance(data, list) and len(data) > 0:
+            data = data[0]
+        return data if isinstance(data, dict) else {}
+    
+    def get_insider(self, ticker: str) -> Dict:
+        """Get insider trading data for a ticker."""
+        raw = self._get(f"/api/insider/{ticker}")
+        data = raw.get("data", {})
+        if isinstance(data, list) and len(data) > 0:
+            data = data[0]
+        return data if isinstance(data, dict) else {}
+    
+    def get_calendar(self, ticker: str) -> Dict:
+        """Get calendar/events data for a ticker."""
+        raw = self._get(f"/api/calendar/{ticker}")
+        data = raw.get("data", {})
+        if isinstance(data, list) and len(data) > 0:
+            data = data[0]
+        return data if isinstance(data, dict) else {}
+    
+    def get_congress(self, ticker: str) -> Dict:
+        """Get congress trading data for a ticker."""
+        raw = self._get(f"/api/congress/{ticker}")
+        data = raw.get("data", {})
+        if isinstance(data, list) and len(data) > 0:
+            data = data[0]
+        return data if isinstance(data, dict) else {}
+    
+    def get_institutional(self, ticker: str) -> Dict:
+        """Get institutional data for a ticker."""
+        raw = self._get(f"/api/institutional/{ticker}")
+        data = raw.get("data", {})
+        if isinstance(data, list) and len(data) > 0:
+            data = data[0]
+        return data if isinstance(data, dict) else {}
+
+
+class SmartPoller:
+    """Intelligent polling manager to optimize API usage."""
+    
+    def __init__(self):
+        self.state_file = StateFiles.SMART_POLLER
+        # OPTIMIZED: Maximize API usage while staying under 15,000/day limit
+        # Market hours: 9:30 AM - 4:00 PM ET = 6.5 hours = 390 minutes
+        # Target: Use ~14,000 calls (93% of limit) to leave buffer
+        #
+        # Calculation:
+        # - Option flow (most critical): 53 tickers ├ù (390/2.5) = 8,268 calls
+        # - Dark pool: 53 tickers ├ù (390/10) = 2,067 calls  
+        # - Greeks: 53 tickers ├ù (390/30) = 689 calls
+        # - Top net impact (market-wide): 390/5 = 78 calls
+        # Total: 8,268 + 2,067 + 689 + 78 = 11,102 calls (74% of limit)
+        #
+        # We can increase frequency if needed, but this is safe
+        self.intervals = {
+            "option_flow": 150,       # 2.5 min: Most critical data, poll frequently
+            "dark_pool_levels": 600,  # 10 min: Important but less time-sensitive
+            "greek_exposure": 1800,   # 30 min: Detailed exposure (changes slowly)
+            "greeks": 1800,           # 30 min: Basic greeks (changes slowly)
+            "top_net_impact": 300,    # 5 min: Market-wide, poll moderately
+            "market_tide": 300,       # 5 min: Market-wide sentiment
+            "insider": 1800,          # 30 min: Insider trading (changes slowly)
+            "calendar": 3600,         # 60 min: Calendar events (changes slowly)
+            "congress": 1800,         # 30 min: Congress trading (changes slowly)
+            "institutional": 1800,    # 30 min: Institutional data (changes slowly)
+            "oi_change": 900,         # 15 min: OI changes per ticker
+            "etf_flow": 1800,         # 30 min: ETF flows per ticker
+            "iv_rank": 1800,          # 30 min: IV rank per ticker
+            "shorts_ftds": 3600,      # 60 min: FTD data changes slowly
+            "max_pain": 900,           # 15 min: Max pain per ticker
+        }
+        self.last_call = self._load_state()
+    
+    def _load_state(self) -> dict:
+        """Load persisted polling timestamps."""
+        try:
+            if self.state_file.exists():
+                return json.loads(self.state_file.read_text())
+        except Exception:
+            pass
+        return {}
+    
+    def _save_state(self):
+        """Persist polling timestamps."""
+        try:
+            self.state_file.parent.mkdir(parents=True, exist_ok=True)
+            tmp = self.state_file.with_suffix(".tmp")
+            tmp.write_text(json.dumps(self.last_call, indent=2))
+            tmp.replace(self.state_file)
+        except Exception:
+            pass
+    
+    def should_poll(self, endpoint: str, force_first: bool = False) -> bool:
+        """Check if enough time has passed since last call."""
+        now = time.time()
+        last = self.last_call.get(endpoint, 0)
+        base_interval = self.intervals.get(endpoint, 60)
+        
+        # #region agent log
+        debug_log("uw_flow_daemon.py:should_poll", "Polling decision", {
+            "endpoint": endpoint,
+            "force_first": force_first,
+            "last": last,
+            "interval": base_interval,
+            "time_since_last": now - last if last > 0 else None
+        }, "H5")
+        # #endregion
+        
+        # If this is the first poll (no last call recorded), allow it immediately
+        if force_first and last == 0:
+            self.last_call[endpoint] = now
+            self._save_state()
+            # #region agent log
+            debug_log("uw_flow_daemon.py:should_poll", "First poll allowed", {"endpoint": endpoint}, "H5")
+            # #endregion
+            return True
+        
+        # OPTIMIZATION: During market hours, use normal intervals
+        # Outside market hours, use longer intervals to conserve quota
+        if self._is_market_hours():
+            interval = base_interval
+        else:
+            # Outside market hours: poll 3x less frequently (conserve quota)
+            interval = base_interval * 3
+        
+        if now - last < interval:
+            # #region agent log
+            debug_log("uw_flow_daemon.py:should_poll", "Polling skipped - interval not elapsed", {
+                "endpoint": endpoint,
+                "time_remaining": interval - (now - last)
+            }, "H5")
+            # #endregion
+            return False
+        
+        # Update timestamp
+        self.last_call[endpoint] = now
+        self._save_state()
+        # #region agent log
+        debug_log("uw_flow_daemon.py:should_poll", "Polling allowed", {"endpoint": endpoint}, "H5")
+        # #endregion
+        return True
+    
+    def _is_market_hours(self) -> bool:
+        """Check if currently in trading hours (9:30 AM - 4:00 PM ET).
+        
+        Uses US/Eastern timezone which automatically handles DST (EST/EDT).
+        Matches timezone usage in main.py and sre_monitoring.py.
+        """
+        try:
+            import pytz
+            et = pytz.timezone('US/Eastern')  # Handles DST automatically (EST/EDT)
+            now_et = datetime.now(et)
+            hour_min = now_et.hour * 60 + now_et.minute
+            market_open = 9 * 60 + 30  # 9:30 AM ET
+            market_close = 16 * 60      # 4:00 PM ET
+            is_open = market_open <= hour_min < market_close
+            
+            # Log market status for debugging (only log when closed to reduce noise)
+            if not is_open:
+                safe_print(f"[UW-DAEMON] Market is CLOSED (ET time: {now_et.strftime('%H:%M')}) - will use longer polling intervals")
+            
+            return is_open
+        except Exception as e:
+            # Maintain backward compatibility: default to True if timezone check fails
+            # This matches original behavior and prevents breaking existing functionality
+            safe_print(f"[UW-DAEMON] ΓÜá∩╕Å  Error checking market hours: {e} - defaulting to OPEN (backward compatibility)")
+            return True
+
+
+class UWFlowDaemon:
+    """Daemon that polls UW API and populates cache."""
+    
+    def __init__(self):
+        self.client = UWClient()
+        self.poller = SmartPoller()
+        self._rate_limited = False  # Track if we've hit rate limit
+        self.tickers = os.getenv("TICKERS", 
+            "AAPL,MSFT,GOOGL,AMZN,META,NVDA,TSLA,AMD,NFLX,INTC,"
+            "SPY,QQQ,IWM,DIA,XLF,XLE,XLK,XLV,XLI,XLP,"
+            "JPM,BAC,GS,MS,C,WFC,BLK,V,MA,"
+            "COIN,PLTR,SOFI,HOOD,RIVN,LCID,F,GM,NIO,"
+            "BA,CAT,XOM,CVX,COP,SLB,"
+            "JNJ,PFE,MRNA,UNH,WMT,TGT,COST,HD,LOW"
+        ).split(",")
+        self.tickers = [t.strip().upper() for t in self.tickers if t.strip()]
+        self.running = True
+        self._shutting_down = False  # Prevent reentrant signal handler calls
+        self._loop_entered = False  # Track if main loop has been entered
+        
+        # Register signal handlers BEFORE any debug_log calls that might block
+        signal.signal(signal.SIGTERM, self._signal_handler)
+        signal.signal(signal.SIGINT, self._signal_handler)
+        
+        # #region agent log
+        try:
+            debug_log("uw_flow_daemon.py:__init__", "UWFlowDaemon initialized", {
+                "ticker_count": len(self.tickers),
+                "has_api_key": bool(self.client.api_key) if hasattr(self, 'client') else False
+            }, "H1")
+        except Exception as debug_err:
+            safe_print(f"[UW-DAEMON] Debug log failed in __init__ (non-critical): {debug_err}")
+        # #endregion
+    
+    def _signal_handler(self, signum, frame):
+        """Handle shutdown signals."""
+        # CRITICAL FIX: Ignore signals until main loop is entered
+        # This prevents premature shutdown during initialization
+        if not self._loop_entered:
+            safe_print(f"[UW-DAEMON] Signal {signum} received before loop entry - IGNORING (daemon still initializing)")
+            return  # Ignore signal until loop is entered
+        
+        # Use safe_print immediately to avoid any blocking
+        safe_print(f"[UW-DAEMON] Signal handler called: signal {signum}")
+        
+        # #region agent log
+        try:
+            debug_log("uw_flow_daemon.py:_signal_handler", "Signal received", {
+                "signum": signum,
+                "signal_name": "SIGTERM" if signum == 15 else "SIGINT" if signum == 2 else f"UNKNOWN({signum})",
+                "already_shutting_down": self._shutting_down,
+                "running_before": self.running,
+                "loop_entered": self._loop_entered
+            }, "H2")
+        except Exception as debug_err:
+            safe_print(f"[UW-DAEMON] Debug log failed in signal handler: {debug_err}")
+        # #endregion
+        
+        # Prevent reentrant calls - if already shutting down, just set flag
+        if self._shutting_down:
+            self.running = False
+            # #region agent log
+            debug_log("uw_flow_daemon.py:_signal_handler", "Already shutting down, setting running=False", {}, "H2")
+            # #endregion
+            return
+        
+        self._shutting_down = True
+        # Use os.write to avoid reentrant print/stderr issues
+        try:
+            import os
+            msg = f"\n[UW-DAEMON] Received signal {signum}, shutting down...\n"
+            os.write(2, msg.encode())  # Write directly to stderr file descriptor (2)
+        except:
+            pass  # If write fails, just continue - we still need to set running=False
+        self.running = False
+        # #region agent log
+        debug_log("uw_flow_daemon.py:_signal_handler", "Signal handled - running set to False", {
+            "running": self.running,
+            "shutting_down": self._shutting_down
+        }, "H2")
+        # #endregion
+    
+    def _normalize_flow_data(self, flow_data: List[Dict], ticker: str) -> Dict:
+        """Normalize flow data into cache format."""
+        if not flow_data:
+            return {}
+        
+        # Calculate sentiment and conviction from flow
+        # API may return "premium" or "total_premium" - try both
+        total_premium = sum(float(t.get("total_premium") or t.get("premium") or 0) for t in flow_data)
+        call_premium = sum(float(t.get("total_premium") or t.get("premium") or 0) for t in flow_data 
+                          if t.get("type", "").upper() in ("CALL", "C"))
+        put_premium = total_premium - call_premium
+        
+        net_premium = call_premium - put_premium
+        
+        # Sentiment based on net premium
+        if net_premium > 100000:
+            sentiment = "BULLISH"
+            conviction = min(1.0, net_premium / 5_000_000)
+        elif net_premium < -100000:
+            sentiment = "BEARISH"
+            conviction = min(1.0, abs(net_premium) / 5_000_000)
+        else:
+            sentiment = "NEUTRAL"
+            conviction = 0.0
+        
+        return {
+            "sentiment": sentiment,
+            "conviction": conviction,
+            "total_premium": total_premium,
+            "call_premium": call_premium,
+            "put_premium": put_premium,
+            "net_premium": net_premium,
+            "trade_count": len(flow_data),
+            "last_update": int(time.time())
+        }
+    
+    def _normalize_dark_pool(self, dp_data: List[Dict]) -> Dict:
+        """Normalize dark pool data."""
+        if not dp_data:
+            return {}
+        
+        total_premium = sum(float(d.get("premium", 0) or 0) for d in dp_data)
+        print_count = len(dp_data)
+        
+        # Sentiment based on premium
+        if total_premium > 1000000:
+            sentiment = "BULLISH"
+        elif total_premium < -1000000:
+            sentiment = "BEARISH"
+        else:
+            sentiment = "NEUTRAL"
+        
+        return {
+            "sentiment": sentiment,
+            "total_premium": total_premium,
+            "print_count": print_count,
+            "last_update": int(time.time())
+        }
+    
+    def _update_cache(self, ticker: str, data: Dict):
+        """Update cache for a ticker."""
+        # #region agent log
+        debug_log("uw_flow_daemon.py:_update_cache", "Cache update start", {
+            "ticker": ticker,
+            "data_keys": list(data.keys()),
+            "has_data": bool(data)
+        }, "H4")
+        # #endregion
+        
+        CACHE_FILE.parent.mkdir(parents=True, exist_ok=True)
+        
+        # Load existing cache
+        cache = {}
+        if CACHE_FILE.exists():
+            try:
+                cache = read_json(CACHE_FILE, default={})
+            except Exception:
+                cache = {}
+        
+        # Update ticker data
+        if ticker not in cache:
+            cache[ticker] = {}
+        
+        # GRACEFUL DEGRADATION: Preserve existing flow_trades if new data is empty
+        # This allows trading bot to continue using stale data when API is rate limited
+        existing_flow_trades = cache[ticker].get("flow_trades", [])
+        new_flow_trades = data.get("flow_trades", [])
+        
+        # If new data is empty but we have existing trades < 2 hours old, preserve them
+        if not new_flow_trades and existing_flow_trades:
+            existing_last_update = cache[ticker].get("_last_update", 0)
+            current_time = time.time()
+            age_sec = current_time - existing_last_update if existing_last_update else float('inf')
+            
+            if age_sec < 2 * 3600:  # Less than 2 hours old
+                print(f"[UW-DAEMON] Preserving existing flow_trades for {ticker} ({int(age_sec/60)} min old, {len(existing_flow_trades)} trades)", flush=True)
+                data["flow_trades"] = existing_flow_trades  # Preserve old trades
+                # Also preserve old sentiment/conviction if new normalization failed
+                if not data.get("sentiment") and cache[ticker].get("sentiment"):
+                    data["sentiment"] = cache[ticker]["sentiment"]
+                if not data.get("conviction") and cache[ticker].get("conviction"):
+                    data["conviction"] = cache[ticker]["conviction"]
+        
+        cache[ticker].update(data)
+        cache[ticker]["_last_update"] = int(time.time())
+        
+        # Add metadata
+        cache["_metadata"] = {
+            "last_update": int(time.time()),
+            "updated_by": "uw_flow_daemon",
+            "ticker_count": len([k for k in cache.keys() if not k.startswith("_")])
+        }
+        
+        # Atomic write
+        atomic_write_json(CACHE_FILE, cache)
+        # #region agent log
+        debug_log("uw_flow_daemon.py:_update_cache", "Cache update complete", {
+            "ticker": ticker,
+            "cache_size": len(cache),
+            "ticker_data_keys": list(cache.get(ticker, {}).keys())
+        }, "H4")
+        # #endregion
+    
+    def _poll_ticker(self, ticker: str):
+        """Poll all endpoints for a ticker."""
+        try:
+            # Check if we're rate limited
+            # If rate limited, we skip NEW polling but keep existing cache data
+            # This allows trading bot to use stale cache (graceful degradation)
+            if hasattr(self, '_rate_limited') and self._rate_limited:
+                # Don't make new API calls, but don't clear existing cache either
+                # Trading bot can use stale data if available
+                return
+            
+            # Poll option flow (should_poll already checks market hours)
+            if self.poller.should_poll("option_flow"):
+                flow_data = self.client.get_option_flow(ticker, limit=100)
+                
+                # Check if rate limited
+                if isinstance(flow_data, dict) and flow_data.get("_rate_limited"):
+                    self._rate_limited = True
+                    # GRACEFUL DEGRADATION: Don't clear existing cache when rate limited
+                    # Preserve old flow_trades so trading bot can use stale data
+                    print(f"[UW-DAEMON] Rate limited for {ticker} - preserving existing cache data", flush=True)
+                    return  # Skip update, keep old cache data
+                
+                if flow_data:
+                    print(f"[UW-DAEMON] Polling {ticker}: got {len(flow_data)} raw trades", flush=True)
+                else:
+                    print(f"[UW-DAEMON] Polling {ticker}: API returned 0 trades", flush=True)
+                
+                flow_normalized = self._normalize_flow_data(flow_data, ticker)
+                
+                # CRITICAL: ALWAYS store flow_trades, even if empty or normalization fails
+                # main.py needs to see the data (or lack thereof) to know what's happening
+                # BUT: If we have existing cache data and API returns empty, preserve old data for graceful degradation
+                existing_cache = {}
+                if CACHE_FILE.exists():
+                    try:
+                        existing_cache = read_json(CACHE_FILE, default={})
+                        existing_ticker_data = existing_cache.get(ticker, {})
+                        existing_flow_trades = existing_ticker_data.get("flow_trades", [])
+                        existing_last_update = existing_ticker_data.get("_last_update", 0)
+                    except:
+                        existing_flow_trades = []
+                        existing_last_update = 0
+                else:
+                    existing_flow_trades = []
+                    existing_last_update = 0
+                
+                # If API returned empty but we have existing trades < 2 hours old, preserve them
+                if not flow_data and existing_flow_trades:
+                    current_time = time.time()
+                    age_sec = current_time - existing_last_update if existing_last_update else float('inf')
+                    if age_sec < 2 * 3600:  # Less than 2 hours old
+                        print(f"[UW-DAEMON] API returned empty for {ticker}, preserving existing cache ({int(age_sec/60)} min old, {len(existing_flow_trades)} trades)", flush=True)
+                        flow_data = existing_flow_trades  # Use existing data
+                        # Re-normalize existing data
+                        flow_normalized = self._normalize_flow_data(flow_data, ticker)
+                
+                cache_update = {
+                    "flow_trades": flow_data if flow_data else []  # Store new data or preserve old
+                }
+                
+                if flow_normalized:
+                    # Add normalized summary data
+                    cache_update.update({
+                        "sentiment": flow_normalized.get("sentiment", "NEUTRAL"),
+                        "conviction": flow_normalized.get("conviction", 0.0),
+                        "total_premium": flow_normalized.get("total_premium", 0.0),
+                        "call_premium": flow_normalized.get("call_premium", 0.0),
+                        "put_premium": flow_normalized.get("put_premium", 0.0),
+                        "net_premium": flow_normalized.get("net_premium", 0.0),
+                        "trade_count": flow_normalized.get("trade_count", 0),
+                        "flow": flow_normalized,  # Also keep nested for compatibility
+                    })
+                else:
+                    # Even if normalization fails, store basic info
+                    cache_update.update({
+                        "sentiment": "NEUTRAL",
+                        "conviction": 0.0,
+                        "trade_count": len(flow_data) if flow_data else 0
+                    })
+                
+                # Always update cache (even if empty - main.py needs to know)
+                # _update_cache will preserve existing data if new is empty (graceful degradation)
+                self._update_cache(ticker, cache_update)
+                
+                # Check what was actually stored (may have preserved old data)
+                final_cache = read_json(CACHE_FILE, default={}) if CACHE_FILE.exists() else {}
+                final_trades = final_cache.get(ticker, {}).get("flow_trades", [])
+                
+                if final_trades:
+                    print(f"[UW-DAEMON] Cache for {ticker}: {len(final_trades)} trades stored", flush=True)
+                else:
+                    print(f"[UW-DAEMON] Cache for {ticker}: empty (no data available)", flush=True)
+            
+            # Poll dark pool
+            if self.poller.should_poll("dark_pool_levels"):
+                dp_data = self.client.get_dark_pool_levels(ticker)
+                dp_normalized = self._normalize_dark_pool(dp_data)
+                if dp_normalized:
+                    # Write dark_pool data (nested is fine - main.py reads it as cache_data.get("dark_pool", {}))
+                    self._update_cache(ticker, {"dark_pool": dp_normalized})
+            
+            # Poll greek_exposure (detailed exposure data)
+            if self.poller.should_poll("greek_exposure"):
+                try:
+                    print(f"[UW-DAEMON] Polling greek_exposure for {ticker}...", flush=True)
+                    gex_data = self.client.get_greek_exposure(ticker)
+                    if gex_data:
+                        # Load existing cache to merge greeks data
+                        cache = read_json(CACHE_FILE, default={}) if CACHE_FILE.exists() else {}
+                        existing_greeks = cache.get(ticker, {}).get("greeks", {})
+                        existing_greeks.update(gex_data)  # Merge with existing greeks data
+                        self._update_cache(ticker, {"greeks": existing_greeks})
+                        print(f"[UW-DAEMON] Updated greek_exposure for {ticker}: {len(gex_data)} fields", flush=True)
+                    else:
+                        print(f"[UW-DAEMON] greek_exposure for {ticker}: API returned empty", flush=True)
+                except Exception as e:
+                    print(f"[UW-DAEMON] Error fetching greek_exposure for {ticker}: {e}", flush=True)
+                    import traceback
+                    print(f"[UW-DAEMON] Traceback: {traceback.format_exc()}", flush=True)
+            
+            # Poll greeks (basic greeks data - separate endpoint)
+            if self.poller.should_poll("greeks"):
+                try:
+                    print(f"[UW-DAEMON] Polling greeks for {ticker}...", flush=True)
+                    greeks_data = self.client.get_greeks(ticker)
+                    if greeks_data:
+                        # Load existing cache to merge greeks data
+                        cache = read_json(CACHE_FILE, default={}) if CACHE_FILE.exists() else {}
+                        existing_greeks = cache.get(ticker, {}).get("greeks", {})
+                        existing_greeks.update(greeks_data)  # Merge with existing
+                        self._update_cache(ticker, {"greeks": existing_greeks})
+                        print(f"[UW-DAEMON] Updated greeks for {ticker}: {len(greeks_data)} fields", flush=True)
+                    else:
+                        print(f"[UW-DAEMON] greeks for {ticker}: API returned empty", flush=True)
+                except Exception as e:
+                    print(f"[UW-DAEMON] Error fetching greeks for {ticker}: {e}", flush=True)
+                    import traceback
+                    print(f"[UW-DAEMON] Traceback: {traceback.format_exc()}", flush=True)
+            
+            # Poll OI change
+            if self.poller.should_poll("oi_change"):
+                try:
+                    print(f"[UW-DAEMON] Polling oi_change for {ticker}...", flush=True)
+                    oi_data = self.client.get_oi_change(ticker)
+                    if oi_data:
+                        self._update_cache(ticker, {"oi_change": oi_data})
+                        print(f"[UW-DAEMON] Updated oi_change for {ticker}: {len(str(oi_data))} bytes", flush=True)
+                    else:
+                        print(f"[UW-DAEMON] oi_change for {ticker}: API returned empty", flush=True)
+                except Exception as e:
+                    print(f"[UW-DAEMON] Error fetching oi_change for {ticker}: {e}", flush=True)
+                    import traceback
+                    print(f"[UW-DAEMON] Traceback: {traceback.format_exc()}", flush=True)
+            
+            # Poll ETF flow
+            if self.poller.should_poll("etf_flow"):
+                try:
+                    print(f"[UW-DAEMON] Polling etf_flow for {ticker}...", flush=True)
+                    etf_data = self.client.get_etf_flow(ticker)
+                    if etf_data:
+                        self._update_cache(ticker, {"etf_flow": etf_data})
+                        print(f"[UW-DAEMON] Updated etf_flow for {ticker}: {len(str(etf_data))} bytes", flush=True)
+                    else:
+                        print(f"[UW-DAEMON] etf_flow for {ticker}: API returned empty", flush=True)
+                except Exception as e:
+                    print(f"[UW-DAEMON] Error fetching etf_flow for {ticker}: {e}", flush=True)
+                    import traceback
+                    print(f"[UW-DAEMON] Traceback: {traceback.format_exc()}", flush=True)
+            
+            # Poll IV rank
+            if self.poller.should_poll("iv_rank"):
+                try:
+                    print(f"[UW-DAEMON] Polling iv_rank for {ticker}...", flush=True)
+                    iv_data = self.client.get_iv_rank(ticker)
+                    if iv_data:
+                        self._update_cache(ticker, {"iv_rank": iv_data})
+                        print(f"[UW-DAEMON] Updated iv_rank for {ticker}: {len(str(iv_data))} bytes", flush=True)
+                    else:
+                        print(f"[UW-DAEMON] iv_rank for {ticker}: API returned empty", flush=True)
+                except Exception as e:
+                    print(f"[UW-DAEMON] Error fetching iv_rank for {ticker}: {e}", flush=True)
+                    import traceback
+                    print(f"[UW-DAEMON] Traceback: {traceback.format_exc()}", flush=True)
+            
+            # Poll shorts/FTDs
+            if self.poller.should_poll("shorts_ftds"):
+                try:
+                    print(f"[UW-DAEMON] Polling shorts_ftds for {ticker}...", flush=True)
+                    ftd_data = self.client.get_shorts_ftds(ticker)
+                    if ftd_data:
+                        self._update_cache(ticker, {"ftd_pressure": ftd_data})
+                        print(f"[UW-DAEMON] Updated ftd_pressure for {ticker}: {len(str(ftd_data))} bytes", flush=True)
+                    else:
+                        print(f"[UW-DAEMON] shorts_ftds for {ticker}: API returned empty", flush=True)
+                except Exception as e:
+                    print(f"[UW-DAEMON] Error fetching shorts_ftds for {ticker}: {e}", flush=True)
+                    import traceback
+                    print(f"[UW-DAEMON] Traceback: {traceback.format_exc()}", flush=True)
+            
+            # Poll max pain
+            if self.poller.should_poll("max_pain"):
+                try:
+                    print(f"[UW-DAEMON] Polling max_pain for {ticker}...", flush=True)
+                    max_pain_data = self.client.get_max_pain(ticker)
+                    if max_pain_data:
+                        # Max pain contributes to greeks_gamma signal
+                        cache = read_json(CACHE_FILE, default={}) if CACHE_FILE.exists() else {}
+                        existing_greeks = cache.get(ticker, {}).get("greeks", {})
+                        max_pain_value = max_pain_data.get("max_pain") or max_pain_data.get("maxPain")
+                        if max_pain_value:
+                            existing_greeks["max_pain"] = max_pain_value
+                            self._update_cache(ticker, {"greeks": existing_greeks})
+                            print(f"[UW-DAEMON] Updated max_pain for {ticker}: {max_pain_value}", flush=True)
+                        else:
+                            print(f"[UW-DAEMON] max_pain for {ticker}: no max_pain value in response (keys: {list(max_pain_data.keys())})", flush=True)
+                    else:
+                        print(f"[UW-DAEMON] max_pain for {ticker}: API returned empty", flush=True)
+                except Exception as e:
+                    print(f"[UW-DAEMON] Error fetching max_pain for {ticker}: {e}", flush=True)
+                    import traceback
+                    print(f"[UW-DAEMON] Traceback: {traceback.format_exc()}", flush=True)
+            
+            # Poll insider
+            if self.poller.should_poll("insider"):
+                try:
+                    print(f"[UW-DAEMON] Polling insider for {ticker}...", flush=True)
+                    insider_data = self.client.get_insider(ticker)
+                    if insider_data:
+                        self._update_cache(ticker, {"insider": insider_data})
+                        print(f"[UW-DAEMON] Updated insider for {ticker}: {len(str(insider_data))} bytes", flush=True)
+                    else:
+                        print(f"[UW-DAEMON] insider for {ticker}: API returned empty", flush=True)
+                except Exception as e:
+                    print(f"[UW-DAEMON] Error fetching insider for {ticker}: {e}", flush=True)
+            
+            # Poll calendar
+            if self.poller.should_poll("calendar"):
+                try:
+                    print(f"[UW-DAEMON] Polling calendar for {ticker}...", flush=True)
+                    calendar_data = self.client.get_calendar(ticker)
+                    if calendar_data:
+                        self._update_cache(ticker, {"calendar": calendar_data})
+                        print(f"[UW-DAEMON] Updated calendar for {ticker}: {len(str(calendar_data))} bytes", flush=True)
+                    else:
+                        print(f"[UW-DAEMON] calendar for {ticker}: API returned empty", flush=True)
+                except Exception as e:
+                    print(f"[UW-DAEMON] Error fetching calendar for {ticker}: {e}", flush=True)
+            
+            # Poll congress
+            if self.poller.should_poll("congress"):
+                try:
+                    print(f"[UW-DAEMON] Polling congress for {ticker}...", flush=True)
+                    congress_data = self.client.get_congress(ticker)
+                    if congress_data:
+                        self._update_cache(ticker, {"congress": congress_data})
+                        print(f"[UW-DAEMON] Updated congress for {ticker}: {len(str(congress_data))} bytes", flush=True)
+                    else:
+                        print(f"[UW-DAEMON] congress for {ticker}: API returned empty", flush=True)
+                except Exception as e:
+                    print(f"[UW-DAEMON] Error fetching congress for {ticker}: {e}", flush=True)
+            
+            # Poll institutional
+            if self.poller.should_poll("institutional"):
+                try:
+                    print(f"[UW-DAEMON] Polling institutional for {ticker}...", flush=True)
+                    institutional_data = self.client.get_institutional(ticker)
+                    if institutional_data:
+                        self._update_cache(ticker, {"institutional": institutional_data})
+                        print(f"[UW-DAEMON] Updated institutional for {ticker}: {len(str(institutional_data))} bytes", flush=True)
+                    else:
+                        print(f"[UW-DAEMON] institutional for {ticker}: API returned empty", flush=True)
+                except Exception as e:
+                    print(f"[UW-DAEMON] Error fetching institutional for {ticker}: {e}", flush=True)
+        
+        except Exception as e:
+            print(f"[UW-DAEMON] Error polling {ticker}: {e}", flush=True)
+    
+    def run(self):
+        """Main daemon loop."""
+        try:
+            safe_print("[UW-DAEMON] run() method called")
+            safe_print(f"[UW-DAEMON] self.running = {self.running}")
+            
+            # #region agent log
+            try:
+                debug_log("uw_flow_daemon.py:run", "Daemon starting", {
+                    "ticker_count": len(self.tickers),
+                    "has_api_key": bool(self.client.api_key),
+                    "cache_file": str(CACHE_FILE)
+                }, "H2")
+            except Exception as debug_err:
+                safe_print(f"[UW-DAEMON] Debug log failed (non-critical): {debug_err}")
+            # #endregion
+            
+            safe_print("[UW-DAEMON] Starting UW Flow Daemon...")
+            safe_print(f"[UW-DAEMON] Monitoring {len(self.tickers)} tickers")
+            safe_print(f"[UW-DAEMON] Cache file: {CACHE_FILE}")
+            
+            # Force first poll of market-wide endpoints on startup
+            first_poll = True
+            cycle = 0
+            
+            safe_print("[UW-DAEMON] Step 1: Variables initialized")
+            safe_print(f"[UW-DAEMON] Step 2: Running flag = {self.running}")
+            
+            # CRITICAL: Check running flag BEFORE any debug_log calls
+            if not self.running:
+                safe_print("[UW-DAEMON] ERROR: running=False before entering loop!")
+                return
+            
+            safe_print("[UW-DAEMON] Step 3: Running check passed")
+            
+            # #region agent log
+            try:
+                debug_log("uw_flow_daemon.py:run", "Entering main loop", {"running": self.running, "cycle": cycle}, "H2")
+            except Exception as debug_err:
+                safe_print(f"[UW-DAEMON] Debug log failed (non-critical): {debug_err}")
+            # #endregion
+            
+            safe_print("[UW-DAEMON] Step 4: About to enter while loop")
+            safe_print(f"[UW-DAEMON] Step 5: Checking while condition: self.running = {self.running}")
+            
+            # CRITICAL: Force check running flag one more time right before loop
+            if not self.running:
+                safe_print("[UW-DAEMON] ERROR: running became False right before loop!")
+                return
+            
+            safe_print("[UW-DAEMON] Step 5.5: Final check passed, entering while loop NOW")
+            
+            # Use a local variable to track if we should continue, to avoid signal handler race conditions
+            should_continue = True
+            
+            # CRITICAL FIX: Enter loop FIRST, then set flag to prevent race condition
+            # This ensures we're actually in the loop before accepting signals
+            while should_continue and self.running:
+                # Set loop entry flag on FIRST iteration only
+                if not self._loop_entered:
+                    self._loop_entered = True
+                    safe_print("[UW-DAEMON] Γ£à LOOP ENTERED - Loop entry flag set, signals will now be honored")
+                
+                safe_print(f"[UW-DAEMON] Step 6: INSIDE while loop! Cycle will be {cycle + 1}")
+                try:
+                    cycle += 1
+                    if cycle == 1:
+                        safe_print(f"[UW-DAEMON] Γ£à SUCCESS: Entered main loop! Cycle {cycle}")
+                    elif cycle <= 3:
+                        safe_print(f"[UW-DAEMON] Loop continuing, cycle {cycle}")
+                    
+                    # Check running flag at start of each cycle
+                    if not self.running:
+                        safe_print(f"[UW-DAEMON] Running flag became False during cycle {cycle}")
+                        should_continue = False
+                        break
+                    
+                    # #region agent log
+                    try:
+                        debug_log("uw_flow_daemon.py:run", "Cycle start", {"cycle": cycle, "first_poll": first_poll, "running": self.running}, "H2")
+                    except Exception as debug_err:
+                        pass  # Non-critical
+                    # #endregion
+                    
+                    # Check if we should exit
+                    if not self.running:
+                        # #region agent log
+                        debug_log("uw_flow_daemon.py:run", "Exiting loop - running=False", {}, "H2")
+                        # #endregion
+                        break
+                    
+                    # Poll top net impact (market-wide, not per-ticker)
+                    if self.poller.should_poll("top_net_impact", force_first=first_poll):
+                        try:
+                            safe_print(f"[UW-DAEMON] Polling top_net_impact (first_poll={first_poll})...")
+                            top_net = self.client.get_top_net_impact(limit=100)
+                            # Store in cache metadata
+                            cache = read_json(CACHE_FILE, default={}) if CACHE_FILE.exists() else {}
+                            cache["_top_net_impact"] = {
+                                "data": top_net,
+                                "last_update": int(time.time())
+                            }
+                            atomic_write_json(CACHE_FILE, cache)
+                        except Exception as e:
+                            safe_print(f"[UW-DAEMON] Error polling top_net_impact: {e}")
+                    
+                    # Poll market tide (market-wide, not per-ticker)
+                    if self.poller.should_poll("market_tide", force_first=first_poll):
+                        try:
+                            safe_print(f"[UW-DAEMON] Polling market_tide (first_poll={first_poll})...")
+                            # #region agent log
+                            debug_log("uw_flow_daemon.py:run:market_tide", "Calling get_market_tide", {"first_poll": first_poll}, "H3")
+                            # #endregion
+                            tide_data = self.client.get_market_tide()
+                            # #region agent log
+                            debug_log("uw_flow_daemon.py:run:market_tide", "get_market_tide response", {
+                                "has_data": bool(tide_data),
+                                "data_type": type(tide_data).__name__,
+                                "data_keys": list(tide_data.keys()) if isinstance(tide_data, dict) else [],
+                                "data_str": str(tide_data)[:200] if tide_data else "empty"
+                            }, "H3")
+                            # #endregion
+                            if tide_data:
+                                # Store in cache metadata
+                                cache = read_json(CACHE_FILE, default={}) if CACHE_FILE.exists() else {}
+                                cache["_market_tide"] = {
+                                    "data": tide_data,
+                                    "last_update": int(time.time())
+                                }
+                                atomic_write_json(CACHE_FILE, cache)
+                                safe_print(f"[UW-DAEMON] Updated market_tide: {len(str(tide_data))} bytes")
+                            else:
+                                safe_print(f"[UW-DAEMON] market_tide: API returned empty data")
+                        except Exception as e:
+                            safe_print(f"[UW-DAEMON] Error polling market_tide: {e}")
+                            import traceback
+                            safe_print(f"[UW-DAEMON] Traceback: {traceback.format_exc()}")
+                    
+                    # Poll each ticker (optimized delay for rate limit efficiency)
+                    for ticker in self.tickers:
+                        if not self.running:
+                            break
+                        self._poll_ticker(ticker)
+                        # 1.5s delay: balances speed with rate limit safety
+                        # With 53 tickers: ~80 seconds per full cycle at 1.5s delay
+                        time.sleep(1.5)
+                    
+                    # Clear first_poll flag after first cycle
+                    if first_poll:
+                        first_poll = False
+                        safe_print("[UW-DAEMON] Completed first poll cycle - all endpoints attempted")
+                    
+                    # Log cycle completion
+                    if cycle % 10 == 0:
+                        safe_print(f"[UW-DAEMON] Completed {cycle} cycles")
+                        # #region agent log
+                        debug_log("uw_flow_daemon.py:run", "Cycle milestone", {"cycle": cycle}, "H2")
+                        # #endregion
+                    
+                    # Sleep before next cycle
+                    # If rate limited, sleep longer (check every 5 minutes for reset)
+                    if self._rate_limited:
+                        # Log status periodically so user knows system is still monitoring
+                        if cycle % 12 == 0:  # Every 12 cycles = every hour when rate limited
+                            safe_print(f"[UW-DAEMON] ΓÅ│ Rate limited - monitoring for reset (8PM EST). Cache data preserved for graceful degradation.")
+                        # #region agent log
+                        debug_log("uw_flow_daemon.py:run", "Rate limited - sleeping", {}, "H2")
+                        # #endregion
+                        time.sleep(300)  # 5 minutes
+                        # Check if it's past 8PM EST (limit reset time)
+                        try:
+                            import pytz
+                            et = pytz.timezone('US/Eastern')
+                            now_et = datetime.now(et)
+                            if now_et.hour >= 20:  # 8PM or later
+                                print(f"[UW-DAEMON] Γ£à Limit should have reset, resuming polling...", flush=True)
+                                self._rate_limited = False
+                        except:
+                            pass
+                    else:
+                        # #region agent log
+                        debug_log("uw_flow_daemon.py:run", "Normal sleep", {"cycle": cycle}, "H2")
+                        # #endregion
+                        time.sleep(30)  # Normal: Check every 30 seconds
+                
+                except KeyboardInterrupt:
+                    safe_print("[UW-DAEMON] Keyboard interrupt received")
+                    # #region agent log
+                    try:
+                        debug_log("uw_flow_daemon.py:run", "Keyboard interrupt", {}, "H2")
+                    except:
+                        pass
+                    # #endregion
+                    should_continue = False
+                    self.running = False
+                    break
+                except Exception as e:
+                    # #region agent log
+                    try:
+                        debug_log("uw_flow_daemon.py:run", "Main loop exception", {
+                            "error": str(e),
+                            "error_type": type(e).__name__,
+                            "cycle": cycle,
+                            "running": self.running
+                        }, "H2")
+                    except:
+                        pass
+                    # #endregion
+                    safe_print(f"[UW-DAEMON] Error in main loop: {e}")
+                    import traceback
+                    tb = traceback.format_exc()
+                    safe_print(f"[UW-DAEMON] Traceback: {tb}")
+                    # #region agent log
+                    try:
+                        debug_log("uw_flow_daemon.py:run", "Exception traceback", {"traceback": tb}, "H2")
+                    except:
+                        pass
+                    # #endregion
+                    # Don't exit on error - continue loop unless explicitly stopped
+                    if not self.running:
+                        safe_print(f"[UW-DAEMON] Running flag False after exception, breaking loop")
+                        should_continue = False
+                        break
+                    time.sleep(60)  # Wait longer on error
+        
+        except Exception as e:
+            safe_print(f"[UW-DAEMON] FATAL ERROR in run() method: {e}")
+            import traceback
+            safe_print(f"[UW-DAEMON] Traceback: {traceback.format_exc()}")
+            # #region agent log
+            try:
+                debug_log("uw_flow_daemon.py:run", "Fatal exception", {
+                    "error": str(e),
+                    "error_type": type(e).__name__,
+                    "traceback": traceback.format_exc()
+                }, "H1")
+            except:
+                pass
+            # #endregion
+            raise
+        
+        safe_print("[UW-DAEMON] Shutting down...")
+        # #region agent log
+        try:
+            debug_log("uw_flow_daemon.py:run", "Daemon shutdown complete", {"cycle": cycle}, "H2")
+        except:
+            pass
+        # #endregion
+        
+        # Reset loop entry flag for potential restart
+        self._loop_entered = False
+
+
+def main():
+    """Entry point."""
+    safe_print("[UW-DAEMON] Main function called")
+    # #region agent log
+    try:
+        debug_log("uw_flow_daemon.py:main", "Main function called", {
+            "cwd": str(Path.cwd()),
+            "script_path": str(Path(__file__)),
+            "debug_log_path": str(DEBUG_LOG_PATH)
+        }, "H1")
+    except Exception as debug_err:
+        safe_print(f"[UW-DAEMON] Debug log failed (non-critical): {debug_err}")
+    # #endregion
+    
+    try:
+        safe_print("[UW-DAEMON] Creating daemon object...")
+        daemon = UWFlowDaemon()
+        safe_print("[UW-DAEMON] Daemon object created successfully")
+        safe_print(f"[UW-DAEMON] Daemon running flag: {daemon.running}")
+        # #region agent log
+        try:
+            debug_log("uw_flow_daemon.py:main", "Daemon object created", {
+                "ticker_count": len(daemon.tickers),
+                "running": daemon.running
+            }, "H1")
+        except Exception as debug_err:
+            safe_print(f"[UW-DAEMON] Debug log failed (non-critical): {debug_err}")
+        # #endregion
+        safe_print("[UW-DAEMON] Calling daemon.run()...")
+        daemon.run()
+        safe_print("[UW-DAEMON] daemon.run() returned")
+    except Exception as e:
+        # #region agent log
+        debug_log("uw_flow_daemon.py:main", "Main exception", {
+            "error": str(e),
+            "error_type": type(e).__name__
+        }, "H1")
+        # #endregion
+        import traceback
+        print(f"[UW-DAEMON] Fatal error: {e}", flush=True)
+        print(f"[UW-DAEMON] Traceback: {traceback.format_exc()}", flush=True)
+        raise
+
+
+if __name__ == "__main__":
+    main()
+
+
